{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LiSTuKXSZ7Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make Fold\n",
    "\n",
    "제가 데이터를 다운로드 하여 놓은 경로는 \"/workspace/Dacon/\" 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QMJAq0a5SZ7e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CzVUkwpCSZ7g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat3'].values)\n",
    "df['cat3'] = le.transform(df['cat3'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYAFNucxSZ7g",
    "outputId": "beea143a-f304-4002-a012-ac8e4d445eaa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['5일장', 'ATV', 'MTB', '강', '게스트하우스', '계곡', '고궁', '고택', '골프', '공연장',\n",
       "       '공예,공방', '공원', '관광단지', '국립공원', '군립공원', '기념관', '기념탑/기념비/전망대',\n",
       "       '기암괴석', '기타', '기타행사', '농.산.어촌 체험', '다리/대교', '대중콘서트', '대형서점',\n",
       "       '도립공원', '도서관', '동굴', '동상', '등대', '래프팅', '면세점', '모텔', '문', '문화관광축제',\n",
       "       '문화원', '문화전수시설', '뮤지컬', '미술관/화랑', '민물낚시', '민박', '민속마을', '바/까페',\n",
       "       '바다낚시', '박람회', '박물관', '발전소', '백화점', '번지점프', '복합 레포츠', '분수', '빙벽등반',\n",
       "       '사격장', '사찰', '산', '상설시장', '생가', '서비스드레지던스', '서양식', '섬', '성',\n",
       "       '수련시설', '수목원', '수상레포츠', '수영', '스노쿨링/스킨스쿠버다이빙', '스카이다이빙', '스케이트',\n",
       "       '스키(보드) 렌탈샵', '스키/스노보드', '승마', '식음료', '썰매장', '안보관광', '야영장,오토캠핑장',\n",
       "       '약수터', '연극', '영화관', '온천/욕장/스파', '외국문화원', '요트', '윈드서핑/제트스키',\n",
       "       '유람선/잠수함관광', '유명건물', '유스호스텔', '유원지', '유적지/사적지', '이색거리', '이색찜질방',\n",
       "       '이색체험', '인라인(실내 인라인 포함)', '일반축제', '일식', '자동차경주', '자연생태관광지',\n",
       "       '자연휴양림', '자전거하이킹', '전문상가', '전시관', '전통공연', '종교성지', '중식', '채식전문점',\n",
       "       '카약/카누', '카지노', '카트', '컨벤션', '컨벤션센터', '콘도미니엄', '클래식음악회', '클럽',\n",
       "       '터널', '테마공원', '트래킹', '특산물판매점', '패밀리레스토랑', '펜션', '폭포', '학교', '한식',\n",
       "       '한옥스테이', '항구/포구', '해수욕장', '해안절경', '헬스투어', '헹글라이딩/패러글라이딩', '호수',\n",
       "       '홈스테이', '희귀동.식물'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D6CT7HwPSZ7i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat2'].values)\n",
    "df['cat2'] = le.transform(df['cat2'].values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat1'].values)\n",
    "df['cat1'] = le.transform(df['cat1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8z0hIVbSZ7i",
    "outputId": "f3552149-23bd-47d2-c058-32de2a069912",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "df['kfold'] = -1\n",
    "for i in range(5):\n",
    "    df_idx, valid_idx = list(folds.split(df.values, df['cat3']))[i]\n",
    "    valid = df.iloc[valid_idx]\n",
    "\n",
    "    df.loc[df[df.id.isin(valid.id) == True].index.to_list(), 'kfold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UKdVurbYSZ7i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('./train_folds.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyjdE1KdSZ7j",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "DdDdtwo3SZ7j",
    "outputId": "c724d17a-5915-4623-99b5-06dc294662a3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                id                       img_path  \\\n",
       "0      TRAIN_00000  ./image/train/TRAIN_00000.jpg   \n",
       "1      TRAIN_00001  ./image/train/TRAIN_00001.jpg   \n",
       "2      TRAIN_00002  ./image/train/TRAIN_00002.jpg   \n",
       "3      TRAIN_00003  ./image/train/TRAIN_00003.jpg   \n",
       "4      TRAIN_00004  ./image/train/TRAIN_00004.jpg   \n",
       "...            ...                            ...   \n",
       "16981  TRAIN_16981  ./image/train/TRAIN_16981.jpg   \n",
       "16982  TRAIN_16982  ./image/train/TRAIN_16982.jpg   \n",
       "16983  TRAIN_16983  ./image/train/TRAIN_16983.jpg   \n",
       "16984  TRAIN_16984  ./image/train/TRAIN_16984.jpg   \n",
       "16985  TRAIN_16985  ./image/train/TRAIN_16985.jpg   \n",
       "\n",
       "                                                overview  cat1  cat2  cat3  \\\n",
       "0      소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 ...     5    13   120   \n",
       "1      경기도 이천시 모가면에 있는 골프장으로 대중제 18홀이다. 회원제로 개장을 했다가 ...     0    11     8   \n",
       "2      금오산성숯불갈비는 한우고기만을 전문적으로 취급하고 사용하는 부식 자재 또한 유기농법...     3    12   118   \n",
       "3      철판 위에서 요리하는 안동찜닭을 맛볼 수 있는 곳이다. 경상북도 안동시에 있는 한식...     3    12   118   \n",
       "4      ※ 영업시간 10:30 ~ 20:30\\n\\n3대에 걸쳐 아귀만을 전문으로 취급하는 ...     3    12   118   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "16981  해발 12000m에 자리한 식담겸 카페점문점이다.<br>곤드레밥과 감자전을 판매하고...     3    12   118   \n",
       "16982  설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...     2     9    31   \n",
       "16983  충남 서산시 중심가에 위치한 줌모텔은 프라이버스가 보장되는 조용한 공간으로 가치가 ...     2     9    31   \n",
       "16984  토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...     0    11    73   \n",
       "16985  포천의 진산으로 불리우는 왕방산(王訪山)에는 천년의 역사를 간직하고 있는 왕산사(王...     4    10    52   \n",
       "\n",
       "       kfold  \n",
       "0          4  \n",
       "1          4  \n",
       "2          4  \n",
       "3          3  \n",
       "4          1  \n",
       "...      ...  \n",
       "16981      0  \n",
       "16982      1  \n",
       "16983      4  \n",
       "16984      1  \n",
       "16985      3  \n",
       "\n",
       "[16986 rows x 7 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-24bd7c31-cd31-4c1b-a82f-977972a2a733\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>overview</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>./image/train/TRAIN_00000.jpg</td>\n",
       "      <td>소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>./image/train/TRAIN_00001.jpg</td>\n",
       "      <td>경기도 이천시 모가면에 있는 골프장으로 대중제 18홀이다. 회원제로 개장을 했다가 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>./image/train/TRAIN_00002.jpg</td>\n",
       "      <td>금오산성숯불갈비는 한우고기만을 전문적으로 취급하고 사용하는 부식 자재 또한 유기농법...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>./image/train/TRAIN_00003.jpg</td>\n",
       "      <td>철판 위에서 요리하는 안동찜닭을 맛볼 수 있는 곳이다. 경상북도 안동시에 있는 한식...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>./image/train/TRAIN_00004.jpg</td>\n",
       "      <td>※ 영업시간 10:30 ~ 20:30\\n\\n3대에 걸쳐 아귀만을 전문으로 취급하는 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16981</th>\n",
       "      <td>TRAIN_16981</td>\n",
       "      <td>./image/train/TRAIN_16981.jpg</td>\n",
       "      <td>해발 12000m에 자리한 식담겸 카페점문점이다.&lt;br&gt;곤드레밥과 감자전을 판매하고...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16982</th>\n",
       "      <td>TRAIN_16982</td>\n",
       "      <td>./image/train/TRAIN_16982.jpg</td>\n",
       "      <td>설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16983</th>\n",
       "      <td>TRAIN_16983</td>\n",
       "      <td>./image/train/TRAIN_16983.jpg</td>\n",
       "      <td>충남 서산시 중심가에 위치한 줌모텔은 프라이버스가 보장되는 조용한 공간으로 가치가 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16984</th>\n",
       "      <td>TRAIN_16984</td>\n",
       "      <td>./image/train/TRAIN_16984.jpg</td>\n",
       "      <td>토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16985</th>\n",
       "      <td>TRAIN_16985</td>\n",
       "      <td>./image/train/TRAIN_16985.jpg</td>\n",
       "      <td>포천의 진산으로 불리우는 왕방산(王訪山)에는 천년의 역사를 간직하고 있는 왕산사(王...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16986 rows × 7 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24bd7c31-cd31-4c1b-a82f-977972a2a733')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-24bd7c31-cd31-4c1b-a82f-977972a2a733 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-24bd7c31-cd31-4c1b-a82f-977972a2a733');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "df = pd.read_csv('./train_folds.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MdfB8iUSSZ7k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CategoryDataset(Dataset):\n",
    "  def __init__(self, text,cats1, cats2, cats3, tokenizer,  max_len):\n",
    "    self.text = text\n",
    "    self.cats1 = cats1\n",
    "    self.cats2 = cats2\n",
    "    self.cats3 = cats3\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.text[item])\n",
    "   \n",
    "   \n",
    "    cat = self.cats1[item]\n",
    "    cat2 = self.cats2[item]\n",
    "    cat3 = self.cats3[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "\n",
    "    return {\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'cats1': torch.tensor(cat, dtype=torch.long),\n",
    "      'cats2': torch.tensor(cat2, dtype=torch.long),\n",
    "      'cats3': torch.tensor(cat3, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size, shuffle_=False):\n",
    "    ds = CategoryDataset(\n",
    "        text=df.overview.to_numpy(),\n",
    "        cats1=df.cat1.to_numpy(),\n",
    "        cats2=df.cat2.to_numpy(),\n",
    "        cats3=df.cat3.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle = shuffle_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AZ-bXMRcSZ7k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TourClassifier(nn.Module):\n",
    "  def __init__(self, n_classes1, n_classes2, n_classes3, text_model_name):\n",
    "    super(TourClassifier, self).__init__()\n",
    "    self.text_model = AutoModel.from_pretrained(text_model_name).to(device)\n",
    "    \n",
    "    self.text_model.gradient_checkpointing_enable()  \n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def get_cls(target_size):\n",
    "      return nn.Sequential(\n",
    "          nn.Linear(self.text_model.config.hidden_size, self.text_model.config.hidden_size),\n",
    "          nn.LayerNorm(self.text_model.config.hidden_size),\n",
    "          nn.Dropout(p = 0.1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(self.text_model.config.hidden_size, target_size),\n",
    "      )  \n",
    "    self.cls = get_cls(n_classes1)\n",
    "    self.cls2 = get_cls(n_classes2)\n",
    "    self.cls3 = get_cls(n_classes3)\n",
    "    \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    # image_output = self.image_model(pixel_values = pixel_values)\n",
    "    # concat_outputs = torch.cat([text_output.last_hidden_state, image_output.last_hidden_state],1)\n",
    "    #config hidden size 일치해야함\n",
    "    # encoder layer는 지우면안된다 ,,, 지웠었네\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=self.text_model.config.hidden_size , nhead=8).to(device)\n",
    "    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2).to(device)\n",
    "\n",
    "    outputs = transformer_encoder(text_output.last_hidden_state)\n",
    "    #cls token \n",
    "    outputs = outputs[:,0]\n",
    "    output = self.drop(outputs)\n",
    "\n",
    "    out1 = self.cls(output)\n",
    "    out2 = self.cls2(output)\n",
    "    out3 = self.cls3(output)\n",
    "    return out1,out2,out3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ku-ThaH6SZ7l",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def calc_tour_acc(pred, label):\n",
    "    _, idx = pred.max(1)\n",
    "    \n",
    "    acc = torch.eq(idx, label).sum().item() / idx.size()[0] \n",
    "    x = label.cpu().numpy()\n",
    "    y = idx.cpu().numpy()\n",
    "    f1_acc = f1_score(x, y, average='weighted')\n",
    "    return acc,f1_acc\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN8udgpLVpeR",
    "outputId": "4e756d89-f862-4295-9239-fe96293f3303",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 5.5 MB 5.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 7.6 MB 56.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001B[K     |████████████████████████████████| 163 kB 57.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.24.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SJ4JKDHnSZ7m",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "import argparse\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples,epoch):\n",
    "\n",
    "  batch_time = AverageMeter()     \n",
    "  data_time = AverageMeter()      \n",
    "  losses = AverageMeter()         \n",
    "  accuracies = AverageMeter()\n",
    "  f1_accuracies = AverageMeter()\n",
    "  \n",
    "  sent_count = AverageMeter()   \n",
    "    \n",
    "\n",
    "  start = end = time.time()\n",
    "\n",
    "  model = model.train()\n",
    "  correct_predictions = 0\n",
    "  for step,d in enumerate(data_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "    batch_size = d[\"input_ids\"].size(0) \n",
    "\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    cats1 = d[\"cats1\"].to(device)\n",
    "    cats2 = d[\"cats2\"].to(device)\n",
    "    cats3 = d[\"cats3\"].to(device)\n",
    "\n",
    "    outputs,outputs2,outputs3 = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    _, preds = torch.max(outputs3, dim=1)\n",
    "\n",
    "    loss1 = loss_fn(outputs, cats1)\n",
    "    loss2 = loss_fn(outputs2, cats2)\n",
    "    loss3 = loss_fn(outputs3, cats3)\n",
    "\n",
    "    loss = loss1 * 0.05 + loss2 * 0.1 + loss3 * 0.85\n",
    "\n",
    "    correct_predictions += torch.sum(preds == cats3)\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    sent_count.update(batch_size)\n",
    "    if step % 200 == 0 or step == (len(data_loader)-1):\n",
    "                acc,f1_acc = calc_tour_acc(outputs3, cats3)\n",
    "                accuracies.update(acc, batch_size)\n",
    "                f1_accuracies.update(f1_acc, batch_size)\n",
    "\n",
    "                \n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.3f}({loss.avg:.3f}) '\n",
    "                      'Acc: {acc.val:.3f}({acc.avg:.3f}) '   \n",
    "                      'f1_Acc: {f1_acc.val:.3f}({f1_acc.avg:.3f}) '           \n",
    "                      'sent/s {sent_s:.0f} '\n",
    "                      .format(\n",
    "                      epoch, step+1, len(data_loader),\n",
    "                      data_time=data_time, loss=losses,\n",
    "                      acc=accuracies,\n",
    "                      f1_acc=f1_accuracies,\n",
    "                      remain=timeSince(start, float(step+1)/len(data_loader)),\n",
    "                      sent_s=sent_count.avg/batch_time.avg\n",
    "                      ))\n",
    "\n",
    "  return correct_predictions.double() / n_examples, losses.avg\n",
    "\n",
    "def validate(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  cnt = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    with torch.no_grad():\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      cats1 = d[\"cats1\"].to(device)\n",
    "      cats2 = d[\"cats2\"].to(device)\n",
    "      cats3 = d[\"cats3\"].to(device)\n",
    "      outputs,outputs2,outputs3 = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs3, dim=1)\n",
    "      loss1 = loss_fn(outputs, cats1)\n",
    "      loss2 = loss_fn(outputs2, cats2)\n",
    "      loss3 = loss_fn(outputs3, cats3)\n",
    "\n",
    "      loss = loss1 * 0.05 + loss2 * 0.1 + loss3 * 0.85\n",
    "\n",
    "      correct_predictions += torch.sum(preds == cats3)\n",
    "      losses.append(loss.item())\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "      if cnt == 0:\n",
    "        cnt +=1\n",
    "        outputs3_arr = outputs3\n",
    "        cats3_arr = cats3\n",
    "      else:\n",
    "        outputs3_arr = torch.cat([outputs3_arr, outputs3],0)\n",
    "        cats3_arr = torch.cat([cats3_arr, cats3],0)\n",
    "  acc,f1_acc = calc_tour_acc(outputs3_arr, cats3_arr)\n",
    "  return f1_acc, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFF6EPhBWf6J",
    "outputId": "ef5f914a-80cb-4abc-e00a-fc85d85c807a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xovggtY4SZ7n",
    "outputId": "2fd15fc3-c5dd-4346-e3cb-ae2f1e30fb00",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train = df[df[\"kfold\"] != 0].reset_index(drop=True)\n",
    "valid = df[df[\"kfold\"] == 0].reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "train_data_loader = create_data_loader(train, tokenizer, 256, 16, shuffle_=True)\n",
    "valid_data_loader = create_data_loader(valid, tokenizer,  256, 16)\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "model = TourClassifier(n_classes1 = 6, n_classes2 = 18, n_classes3 = 128, text_model_name = \"klue/roberta-large\").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr= 3e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "optimizer,\n",
    "num_warmup_steps=int(total_steps*0.1),\n",
    "num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip"
   ],
   "metadata": {
    "id": "aEvPQiCZZReQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7lAk6LISZ7n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7fd8a559-96d9-46fe-ba0f-86bc9813efa8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------\n",
      "Epoch 0/29\n",
      "----------\n",
      "Epoch: [0][1/850] Data 0.015 (0.015) Elapsed 0m 3s (remain 43m 9s) Loss: 4.653(4.653) Acc: 0.000(0.000) f1_Acc: 0.000(0.000) sent/s 5 \n",
      "Epoch: [0][201/850] Data 0.009 (0.009) Elapsed 10m 37s (remain 34m 19s) Loss: 3.938(4.319) Acc: 0.188(0.094) f1_Acc: 0.059(0.030) sent/s 5 \n",
      "Epoch: [0][401/850] Data 0.008 (0.009) Elapsed 21m 11s (remain 23m 43s) Loss: 2.485(3.694) Acc: 0.562(0.250) f1_Acc: 0.479(0.179) sent/s 5 \n",
      "Epoch: [0][601/850] Data 0.011 (0.009) Elapsed 31m 45s (remain 13m 9s) Loss: 1.803(3.146) Acc: 0.750(0.375) f1_Acc: 0.694(0.308) sent/s 5 \n"
     ]
    }
   ],
   "source": [
    "max_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print('-' * 10)\n",
    "    print(f'Epoch {epoch}/{EPOCHS-1}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train),\n",
    "        epoch \n",
    "    )\n",
    "    validate_acc, validate_loss = validate(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(valid)\n",
    "    )\n",
    "\n",
    "    if validate_acc > max_acc:\n",
    "        max_acc = validate_acc\n",
    "        torch.save(model.state_dict(),f'tourbaseline_fold0.pt')\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    print(f'Validate loss {validate_loss} accuracy {validate_acc}')\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    \"\"\"\n",
    "    이것도 비슨하게 상승하네 상승할듯 처음엔 아예 측정안된값이고\n",
    "    심지어 더 빠른듯 코랩프로가 , 내 파이참보다 t4이기도하고  .. 이거네 .... encoder를 이해못해서 지워서그랬나 진짜 하\n",
    "    왜 이거하나 안됐떤거지 코랩프로에서 안해서 그런가\n",
    "\n",
    "Epoch: [0][1/850] Data 0.015 (0.015) Elapsed 0m 3s (remain 43m 9s) Loss: 4.653(4.653) Acc: 0.000(0.000) f1_Acc: 0.000(0.000) sent/s 5 \n",
    "Epoch: [0][201/850] Data 0.009 (0.009) Elapsed 10m 37s (remain 34m 19s) Loss: 3.938(4.319) Acc: 0.188(0.094) f1_Acc: 0.059(0.030) sent/s 5\n",
    "Epoch: [0][401/850] Data 0.008 (0.009) Elapsed 21m 11s (remain 23m 43s) Loss: 2.485(3.694) Acc: 0.562(0.250) f1_Acc: 0.479(0.179) sent/s 5 \n",
    "Epoch: [0][601/850] Data 0.011 (0.009) Elapsed 31m 45s (remain 13m 9s) Loss: 1.803(3.146) Acc: 0.750(0.375) f1_Acc: 0.694(0.308) sent/s 5\n",
    "왜 이거 하날 못했찌 급한 마음이라 그런가 대회떄는 생각없이 해결하게 된계기는 김빠지게 역시나 확신한 근거를 가지고 생ㄴ각해서 공부해서 해결해봤기 때문\n",
    "encoder를 없애면 안된다 \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7fd8a559-96d9-46fe-ba0f-86bc9813efa8",
    "id": "4HTpiovVbRrh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------\n",
      "Epoch 0/29\n",
      "----------\n",
      "Epoch: [0][1/850] Data 0.015 (0.015) Elapsed 0m 3s (remain 43m 9s) Loss: 4.653(4.653) Acc: 0.000(0.000) f1_Acc: 0.000(0.000) sent/s 5 \n",
      "Epoch: [0][201/850] Data 0.009 (0.009) Elapsed 10m 37s (remain 34m 19s) Loss: 3.938(4.319) Acc: 0.188(0.094) f1_Acc: 0.059(0.030) sent/s 5 \n"
     ]
    }
   ],
   "source": [
    "max_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print('-' * 10)\n",
    "    print(f'Epoch {epoch}/{EPOCHS-1}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train),\n",
    "        epoch \n",
    "    )\n",
    "    validate_acc, validate_loss = validate(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(valid)\n",
    "    )\n",
    "\n",
    "    if validate_acc > max_acc:\n",
    "        max_acc = validate_acc\n",
    "        torch.save(model.state_dict(),f'tourbaseline_fold0.pt')\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    print(f'Validate loss {validate_loss} accuracy {validate_acc}')\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRe4AxLzSZ7n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BBvNnrlSZ7o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CategoryDataset(Dataset):\n",
    "  def __init__(self, text, image_path, tokenizer, feature_extractor, max_len):\n",
    "    self.text = text\n",
    "    self.image_path = image_path\n",
    "    self.tokenizer = tokenizer\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.text[item])\n",
    "    image_path = os.path.join('/workspace/Dacon/data',str(self.image_path[item])[2:])\n",
    "    image = cv2.imread(image_path)\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    image_feature = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    return {\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'pixel_values': image_feature['pixel_values'][0],\n",
    "    }\n",
    "\n",
    "def create_data_loader(df, tokenizer, feature_extractor, max_len, batch_size, shuffle_=False):\n",
    "    ds = CategoryDataset(\n",
    "        text=df.overview.to_numpy(),\n",
    "        image_path = df.img_path.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        feature_extractor = feature_extractor,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle = shuffle_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV-Q4BlRSZ7o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model,data_loader,device,n_examples):\n",
    "  model = model.eval()\n",
    "  preds_arr = []\n",
    "  preds_arr2 = []\n",
    "  preds_arr3 = []\n",
    "  for d in tqdm(data_loader):\n",
    "    with torch.no_grad():\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      pixel_values = d['pixel_values'].to(device)\n",
    "\n",
    "      outputs,outputs2,outputs3 = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pixel_values=pixel_values\n",
    "      )\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      _, preds2 = torch.max(outputs2, dim=1)\n",
    "      _, preds3 = torch.max(outputs3, dim=1)\n",
    "\n",
    "      preds_arr.append(preds.cpu().numpy())\n",
    "      preds_arr2.append(preds2.cpu().numpy())\n",
    "      preds_arr3.append(preds3.cpu().numpy())\n",
    "\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "  return preds_arr, preds_arr2, preds_arr3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWskBvf7SZ7o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/workspace/Dacon/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxJXx0hESZ7o",
    "outputId": "81ad4296-33e4-4148-846d-baa0c56d9547",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7280/7280 [12:04<00:00, 10.05it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_data_loader = create_data_loader(test, tokenizer, feature_extractor, 256, 1)\n",
    "\n",
    "preds_arr, preds_arr2, preds_arr3 = inference(\n",
    "        model,\n",
    "        eval_data_loader,\n",
    "        device,\n",
    "        len(test)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBOp_R1oSZ7p",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('/workspace/Dacon/data/sample_submission.csv')\n",
    "arr = ['5일장', 'ATV', 'MTB', '강', '게스트하우스', '계곡', '고궁', '고택', '골프', '공연장',\n",
    "       '공예,공방', '공원', '관광단지', '국립공원', '군립공원', '기념관', '기념탑/기념비/전망대',\n",
    "       '기암괴석', '기타', '기타행사', '농.산.어촌 체험', '다리/대교', '대중콘서트', '대형서점',\n",
    "       '도립공원', '도서관', '동굴', '동상', '등대', '래프팅', '면세점', '모텔', '문', '문화관광축제',\n",
    "       '문화원', '문화전수시설', '뮤지컬', '미술관/화랑', '민물낚시', '민박', '민속마을', '바/까페',\n",
    "       '바다낚시', '박람회', '박물관', '발전소', '백화점', '번지점프', '복합 레포츠', '분수', '빙벽등반',\n",
    "       '사격장', '사찰', '산', '상설시장', '생가', '서비스드레지던스', '서양식', '섬', '성',\n",
    "       '수련시설', '수목원', '수상레포츠', '수영', '스노쿨링/스킨스쿠버다이빙', '스카이다이빙', '스케이트',\n",
    "       '스키(보드) 렌탈샵', '스키/스노보드', '승마', '식음료', '썰매장', '안보관광', '야영장,오토캠핑장',\n",
    "       '약수터', '연극', '영화관', '온천/욕장/스파', '외국문화원', '요트', '윈드서핑/제트스키',\n",
    "       '유람선/잠수함관광', '유명건물', '유스호스텔', '유원지', '유적지/사적지', '이색거리', '이색찜질방',\n",
    "       '이색체험', '인라인(실내 인라인 포함)', '일반축제', '일식', '자동차경주', '자연생태관광지',\n",
    "       '자연휴양림', '자전거하이킹', '전문상가', '전시관', '전통공연', '종교성지', '중식', '채식전문점',\n",
    "       '카약/카누', '카지노', '카트', '컨벤션', '컨벤션센터', '콘도미니엄', '클래식음악회', '클럽',\n",
    "       '터널', '테마공원', '트래킹', '특산물판매점', '패밀리레스토랑', '펜션', '폭포', '학교', '한식',\n",
    "       '한옥스테이', '항구/포구', '해수욕장', '해안절경', '헬스투어', '헹글라이딩/패러글라이딩', '호수',\n",
    "       '홈스테이', '희귀동.식물']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knUOwdgcSZ7p",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds_arr3)):\n",
    "    sample_submission.loc[i,'cat3'] = arr[preds_arr3[i][0]]\n",
    "\n",
    "sample_submission.to_csv('baseline.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}