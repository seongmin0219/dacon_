{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 다른 거 첨가한다고 결국 재현 안한 (분명 해봤자일거라 생각) 코드\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "all_df = pd.read_csv('./input/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "all_df['cat1'] = le.fit_transform(all_df['cat1'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le.classes_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [],
   "source": [
    "# Dataset 을 상속받고 DataLoader 에 넘겨진다.\n",
    "class CateDataset(Dataset):\n",
    "    def __init__(self , text , cat1 , tokenizer):\n",
    "        \"\"\"\n",
    "        :param text: 관광지 설명\n",
    "        :param cat1: 대 카테고리 cat1\n",
    "        \"\"\"\n",
    "        if not isinstance( text,str):\n",
    "            print('Text must be str')\n",
    "        self.text = text\n",
    "        self.cat1 = cat1\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.enable_padding(\"right\",length=512)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        cat1 = self.cat1[item]\n",
    "        texts  = \"\"\n",
    "        if '.' in text:# 문장 형식으로 되있다면\n",
    "            for sentence in text.split('.'):\n",
    "                texts+= sentence[:51]\n",
    "                if len(texts) >= 512:\n",
    "                    break\n",
    "        else:\n",
    "            texts = text\n",
    "\n",
    "        encoding = self.tokenizer.encode(texts)\n",
    "        # print(torch.tensor(encoding.ids), torch.tensor(encoding.attention_mask), torch.LongTensor(cat1))\n",
    "        return torch.tensor(encoding.ids,dtype=torch.long), torch.tensor(encoding.attention_mask,dtype=torch.long), torch.LongTensor([cat1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "class CateClassifier(nn.Module):\n",
    "    def __init__(self, n_classes_1 ):\n",
    "        super(CateClassifier, self).__init__()\n",
    "        self.text_model = BertForSequenceClassification(BertConfig(vocab_size=42000 ,id2label={\n",
    "    \"0\": \"0\",\n",
    "    \"1\": \"1\",\n",
    "    \"2\": \"10\",\n",
    "    \"3\": \"100\",\n",
    "    \"4\": \"101\",\n",
    "    \"5\": \"102\",\n",
    "    \"6\": \"103\",\n",
    "    \"7\": \"104\",\n",
    "    \"8\": \"105\",\n",
    "    \"9\": \"106\",\n",
    "    \"10\": \"107\",\n",
    "    \"11\": \"108\",\n",
    "    \"12\": \"109\",\n",
    "    \"13\": \"11\",\n",
    "    \"14\": \"110\",\n",
    "    \"15\": \"111\",\n",
    "    \"16\": \"112\",\n",
    "    \"17\": \"113\",\n",
    "    \"18\": \"114\",\n",
    "    \"19\": \"115\",\n",
    "    \"20\": \"116\",\n",
    "    \"21\": \"117\",\n",
    "    \"22\": \"118\",\n",
    "    \"23\": \"119\",\n",
    "    \"24\": \"12\",\n",
    "    \"25\": \"120\",\n",
    "    \"26\": \"121\",\n",
    "    \"27\": \"122\",\n",
    "    \"28\": \"123\",\n",
    "    \"29\": \"124\",\n",
    "    \"30\": \"125\",\n",
    "    \"31\": \"126\",\n",
    "    \"32\": \"127\",\n",
    "    \"33\": \"13\",\n",
    "    \"34\": \"14\",\n",
    "    \"35\": \"15\",\n",
    "    \"36\": \"16\",\n",
    "    \"37\": \"17\",\n",
    "    \"38\": \"18\",\n",
    "    \"39\": \"19\",\n",
    "    \"40\": \"2\",\n",
    "    \"41\": \"20\",\n",
    "    \"42\": \"21\",\n",
    "    \"43\": \"22\",\n",
    "    \"44\": \"23\",\n",
    "    \"45\": \"24\",\n",
    "    \"46\": \"25\",\n",
    "    \"47\": \"26\",\n",
    "    \"48\": \"27\",\n",
    "    \"49\": \"28\",\n",
    "    \"50\": \"29\",\n",
    "    \"51\": \"3\",\n",
    "    \"52\": \"30\",\n",
    "    \"53\": \"31\",\n",
    "    \"54\": \"32\",\n",
    "    \"55\": \"33\",\n",
    "    \"56\": \"34\",\n",
    "    \"57\": \"35\",\n",
    "    \"58\": \"36\",\n",
    "    \"59\": \"37\",\n",
    "    \"60\": \"38\",\n",
    "    \"61\": \"39\",\n",
    "    \"62\": \"4\",\n",
    "    \"63\": \"40\",\n",
    "    \"64\": \"41\",\n",
    "    \"65\": \"42\",\n",
    "    \"66\": \"43\",\n",
    "    \"67\": \"44\",\n",
    "    \"68\": \"45\",\n",
    "    \"69\": \"46\",\n",
    "    \"70\": \"47\",\n",
    "    \"71\": \"48\",\n",
    "    \"72\": \"49\",\n",
    "    \"73\": \"5\",\n",
    "    \"74\": \"50\",\n",
    "    \"75\": \"51\",\n",
    "    \"76\": \"52\",\n",
    "    \"77\": \"53\",\n",
    "    \"78\": \"54\",\n",
    "    \"79\": \"55\",\n",
    "    \"80\": \"56\",\n",
    "    \"81\": \"57\",\n",
    "    \"82\": \"58\",\n",
    "    \"83\": \"59\",\n",
    "    \"84\": \"6\",\n",
    "    \"85\": \"60\",\n",
    "    \"86\": \"61\",\n",
    "    \"87\": \"62\",\n",
    "    \"88\": \"63\",\n",
    "    \"89\": \"64\",\n",
    "    \"90\": \"65\",\n",
    "    \"91\": \"66\",\n",
    "    \"92\": \"67\",\n",
    "    \"93\": \"68\",\n",
    "    \"94\": \"69\",\n",
    "    \"95\": \"7\",\n",
    "    \"96\": \"70\",\n",
    "    \"97\": \"71\",\n",
    "    \"98\": \"72\",\n",
    "    \"99\": \"73\",\n",
    "    \"100\": \"74\",\n",
    "    \"101\": \"75\",\n",
    "    \"102\": \"76\",\n",
    "    \"103\": \"77\",\n",
    "    \"104\": \"78\",\n",
    "    \"105\": \"79\",\n",
    "    \"106\": \"8\",\n",
    "    \"107\": \"80\",\n",
    "    \"108\": \"81\",\n",
    "    \"109\": \"82\",\n",
    "    \"110\": \"83\",\n",
    "    \"111\": \"84\",\n",
    "    \"112\": \"85\",\n",
    "    \"113\": \"86\",\n",
    "    \"114\": \"87\",\n",
    "    \"115\": \"88\",\n",
    "    \"116\": \"89\",\n",
    "    \"117\": \"9\",\n",
    "    \"118\": \"90\",\n",
    "    \"119\": \"91\",\n",
    "    \"120\": \"92\",\n",
    "    \"121\": \"93\",\n",
    "    \"122\": \"94\",\n",
    "    \"123\": \"95\",\n",
    "    \"124\": \"96\",\n",
    "    \"125\": \"97\",\n",
    "    \"126\": \"98\",\n",
    "    \"127\": \"99\"\n",
    "  }  , label2id= {\n",
    "    \"0\": 0,\n",
    "    \"1\": 1,\n",
    "    \"10\": 2,\n",
    "    \"100\": 3,\n",
    "    \"101\": 4,\n",
    "    \"102\": 5,\n",
    "    \"103\": 6,\n",
    "    \"104\": 7,\n",
    "    \"105\": 8,\n",
    "    \"106\": 9,\n",
    "    \"107\": 10,\n",
    "    \"108\": 11,\n",
    "    \"109\": 12,\n",
    "    \"11\": 13,\n",
    "    \"110\": 14,\n",
    "    \"111\": 15,\n",
    "    \"112\": 16,\n",
    "    \"113\": 17,\n",
    "    \"114\": 18,\n",
    "    \"115\": 19,\n",
    "    \"116\": 20,\n",
    "    \"117\": 21,\n",
    "    \"118\": 22,\n",
    "    \"119\": 23,\n",
    "    \"12\": 24,\n",
    "    \"120\": 25,\n",
    "    \"121\": 26,\n",
    "    \"122\": 27,\n",
    "    \"123\": 28,\n",
    "    \"124\": 29,\n",
    "    \"125\": 30,\n",
    "    \"126\": 31,\n",
    "    \"127\": 32,\n",
    "    \"13\": 33,\n",
    "    \"14\": 34,\n",
    "    \"15\": 35,\n",
    "    \"16\": 36,\n",
    "    \"17\": 37,\n",
    "    \"18\": 38,\n",
    "    \"19\": 39,\n",
    "    \"2\": 40,\n",
    "    \"20\": 41,\n",
    "    \"21\": 42,\n",
    "    \"22\": 43,\n",
    "    \"23\": 44,\n",
    "    \"24\": 45,\n",
    "    \"25\": 46,\n",
    "    \"26\": 47,\n",
    "    \"27\": 48,\n",
    "    \"28\": 49,\n",
    "    \"29\": 50,\n",
    "    \"3\": 51,\n",
    "    \"30\": 52,\n",
    "    \"31\": 53,\n",
    "    \"32\": 54,\n",
    "    \"33\": 55,\n",
    "    \"34\": 56,\n",
    "    \"35\": 57,\n",
    "    \"36\": 58,\n",
    "    \"37\": 59,\n",
    "    \"38\": 60,\n",
    "    \"39\": 61,\n",
    "    \"4\": 62,\n",
    "    \"40\": 63,\n",
    "    \"41\": 64,\n",
    "    \"42\": 65,\n",
    "    \"43\": 66,\n",
    "    \"44\": 67,\n",
    "    \"45\": 68,\n",
    "    \"46\": 69,\n",
    "    \"47\": 70,\n",
    "    \"48\": 71,\n",
    "    \"49\": 72,\n",
    "    \"5\": 73,\n",
    "    \"50\": 74,\n",
    "    \"51\": 75,\n",
    "    \"52\": 76,\n",
    "    \"53\": 77,\n",
    "    \"54\": 78,\n",
    "    \"55\": 79,\n",
    "    \"56\": 80,\n",
    "    \"57\": 81,\n",
    "    \"58\": 82,\n",
    "    \"59\": 83,\n",
    "    \"6\": 84,\n",
    "    \"60\": 85,\n",
    "    \"61\": 86,\n",
    "    \"62\": 87,\n",
    "    \"63\": 88,\n",
    "    \"64\": 89,\n",
    "    \"65\": 90,\n",
    "    \"66\": 91,\n",
    "    \"67\": 92,\n",
    "    \"68\": 93,\n",
    "    \"69\": 94,\n",
    "    \"7\": 95,\n",
    "    \"70\": 96,\n",
    "    \"71\": 97,\n",
    "    \"72\": 98,\n",
    "    \"73\": 99,\n",
    "    \"74\": 100,\n",
    "    \"75\": 101,\n",
    "    \"76\": 102,\n",
    "    \"77\": 103,\n",
    "    \"78\": 104,\n",
    "    \"79\": 105,\n",
    "    \"8\": 106,\n",
    "    \"80\": 107,\n",
    "    \"81\": 108,\n",
    "    \"82\": 109,\n",
    "    \"83\": 110,\n",
    "    \"84\": 111,\n",
    "    \"85\": 112,\n",
    "    \"86\": 113,\n",
    "    \"87\": 114,\n",
    "    \"88\": 115,\n",
    "    \"89\": 116,\n",
    "    \"9\": 117,\n",
    "    \"90\": 118,\n",
    "    \"91\": 119,\n",
    "    \"92\": 120,\n",
    "    \"93\": 121,\n",
    "    \"94\": 122,\n",
    "    \"95\": 123,\n",
    "    \"96\": 124,\n",
    "    \"97\": 125,\n",
    "    \"98\": 126,\n",
    "    \"99\": 127\n",
    "  } ,initializer_range=0.02,max_length=192,max_position_embeddings=512,num_attention_heads=12))\n",
    "        # BertModel 을 통해 embedding 을 획득하고 classifier 를 활용하여 획득\n",
    "        self.text_model.num_labels = n_classes_1\n",
    "        self.text_model.classifier.out_features = n_classes_1\n",
    "\n",
    "    def forward(self, input_ids , attention_mask):\n",
    "        \"\"\"__call__ model special method\n",
    "        :param input_ids: model input_ids\n",
    "        :param attention_mask:  model attention_mask\n",
    "        :return: outputs\n",
    "        \"\"\"\n",
    "        outputs = self.text_model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "n_classes_1=  all_df['cat1'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "import re\n",
    "all_df['overview'] =  all_df['overview'].map(lambda x: ' ' .join(re.compile('[ㄱ-힣.]+').findall(x)))\n",
    "#train_df['overview_re'] = train_df['overview'].map(lambda x : ' '.join([t.strip() if len(t) < 51 else t[:51].strip() for t in x.split('.')])[:512])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "def cutting_text(text):\n",
    "    if len(text) > 512:\n",
    "        return ' '.join([t.strip() if len(t) < 51 else t[:51].strip() for t in text.split('.')])[:512]\n",
    "    else:\n",
    "        return  text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "all_df['overview'] = all_df['overview'].map(cutting_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "overviews = all_df['overview'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "# class TextSampler(Sampler):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset , batch_size=1 , shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "MAX_LEN = 512"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=False,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "data": {
      "text/plain": "0        소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 ...\n1        경기도 이천시 모가면에 있는 골프장으로 대중제 홀이다. 회원제로 개장을 했다가 년 ...\n2        금오산성숯불갈비는 한우고기만을 전문적으로 취급하고 사용하는 부식 자재 또한 유기농법...\n3        철판 위에서 요리하는 안동찜닭을 맛볼 수 있는 곳이다. 경상북도 안동시에 있는 한식...\n4            영업시간 대에 걸쳐 아귀만을 전문으로 취급하는 전통과 역사를 자랑하는 음식점이다.\n                               ...                        \n16981            해발 에 자리한 식담겸 카페점문점이다. 곤드레밥과 감자전을 판매하고 있다.\n16982    설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...\n16983    충남 서산시 중심가에 위치한 줌모텔은 프라이버스가 보장되는 조용한 공간으로 가치가 ...\n16984    토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...\n16985    포천의 진산으로 불리우는 왕방산 王訪山 에는 천년의 역사를 간직하고 있는 왕산사 王...\nName: overview, Length: 16986, dtype: object"
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#naver_df = pd.read_table('ratings.txt')\n",
    "#naver_df = naver_df.dropna(how='any')\n",
    "all_df['overview'].dropna(how='any')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "outputs": [],
   "source": [
    "with open('sightseeing_overview.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(all_df['overview']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "outputs": [],
   "source": [
    "data_file = 'sightseeing_overview.txt'\n",
    "vocab_size = 32000\n",
    "# limit_alphabet = 1\n",
    "#limit_alphabet : 병합 전의 초기 토큰의 허용 개수.\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method BaseTokenizer.get_vocab_size of Tokenizer(vocabulary_size=32000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=False, handle_chinese_chars=False, strip_accents=False, lowercase=True, wordpieces_prefix=##)>"
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "cat1 = all_df['cat1'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,valid = train_test_split(all_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [],
   "source": [
    "train_overviews = train['overview']\n",
    "valid_overviews = valid['overview']\n",
    "train_cat1 =train['cat1'].values\n",
    "valid_cat1 = valid['cat1'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [],
   "source": [
    "train_overviews.reset_index(drop=True,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "valid_overviews.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text must be str\n"
     ]
    }
   ],
   "source": [
    "# reset_index가 안되서 keyerror , sampler도 동일한 이유일 수 있다\n",
    "#train_overviews[12386]\n",
    "train_dataset = CateDataset(train_overviews, train_cat1,tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text must be str\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = CateDataset(valid_overviews , valid_cat1, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "# sampler image id를 가지고 가져온다 dataloader 에 넣으면\n",
    "class TextSampler(Sampler):\n",
    "    def __init__(self,\n",
    "                 # sample_idx,\n",
    "                 label_encoder,\n",
    "                 data_source='../sightseeing_ai_hub/input/train.csv'):\n",
    "        super().__init__(data_source)\n",
    "        # self.sample_idx = sample_idx\n",
    "        self.df = pd.read_csv(data_source)\n",
    "        self.df['cat3_enc'] = label_encoder.fit_transform(self.df['cat3'])\n",
    "        self.skf = StratifiedKFold(n_splits=4, shuffle=False)\n",
    "    def __iter__(self):\n",
    "         # overviews = self.df_images['id'].loc[self.sample_idx]\n",
    "         for train_idx, test_idx in self.skf.split(self.df['overview'], self.df['cat3_enc']):\n",
    "            yield train_idx,test_idx\n",
    "        # return self.sample_idx\n",
    "        # for idx in self.sample_idx:\n",
    "            # yield idx\n",
    "        # return iter(overviews)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [],
   "source": [
    "#train_dataloader=  DataLoader(train_dataset , batch_size=16 , shuffle=False, sampler=TextSampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset , batch_size= 1 , shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.CateDataset at 0x1e334575b50>"
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [],
   "source": [
    "valid_dataloader = DataLoader(valid_dataset , batch_size= 1, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method BaseTokenizer.add_special_tokens of Tokenizer(vocabulary_size=42000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=False, handle_chinese_chars=False, strip_accents=False, lowercase=True, wordpieces_prefix=##)>"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "data": {
      "text/plain": "'[PAD]'"
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "data": {
      "text/plain": "'[UNK]'"
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5, 0, 3, 3, 3, 0, 0, 5])"
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BertforSequenceClassification은 너무 오래걸리는듯하다 아니면 영어만되나"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [],
   "source": [
    "import torch_optimizer as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "outputs": [],
   "source": [
    "model = CateClassifier(n_classes_1=len(n_classes_1))\n",
    "optimizer = optim.Lookahead(optim.RAdam(filter(lambda p: p.requires_grad,model.parameters()), lr=5e-5), alpha=0.5, k=5)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=0 ,num_training_steps=len(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0])"
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([cat1[0]])\n",
    "torch.LongTensor(cat1[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[25944,  9051,  8277,  ...,     0,     0,     0],\n         [ 6659, 13514, 34980,  ...,     0,     0,     0],\n         [17015,  3780, 17225,  ...,     0,     0,     0],\n         ...,\n         [17041, 16583,  6816,  ...,     0,     0,     0],\n         [ 3103,  3779,  8930,  ...,     0,     0,     0],\n         [39586, 16398, 15604,  ...,     0,     0,     0]]),\n tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]]),\n tensor([[5],\n         [0],\n         [3],\n         [3],\n         [3],\n         [0],\n         [0],\n         [5]])]"
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "outputs": [],
   "source": [
    "def calc_accuracy(output, label):\n",
    "    # pred = torch.max(output,dim=1)\n",
    "    max_vals, max_indices = torch.max(output,1)\n",
    "    train_acc = (max_indices == label ).sum().data.cpu().numpy() / max_indices.size()[0]\n",
    "    return train_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [506]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     11\u001B[0m output \u001B[38;5;241m=\u001B[39m model(input_ids,attention_mask)\n\u001B[0;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(output\u001B[38;5;241m.\u001B[39mlogits,label[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m---> 13\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     15\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\_tensor.py:307\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    300\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    301\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    305\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    306\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 307\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    152\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m--> 154\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# padding 해야 list of batch must be equal size error 만나지 않는다\n",
    "# 너무 심각하게 오래걸린다..\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    test_acc =0.0\n",
    "    train_acc =0.0\n",
    "    model.train()\n",
    "    for step , (input_ids, attention_mask, label) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model(input_ids,attention_mask)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        output = model(input_ids,attention_mask)\n",
    "        loss = loss_fn(output.logits,label[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(\"epoch {} train acc {}\".format(e+1,train_acc/(step+1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_id, (token_ids, attention_mask, label) in enumerate(tqdm_notebook(valid_dataloader)):\n",
    "    token_ids = token_ids\n",
    "    output = model(input_ids,attention_mask)\n",
    "    test_acc += calc_accuracy(output.logits , label)\n",
    "    if batch_id ==50:\n",
    "        break\n",
    "print(\"epoch {} test acc {}\".format(e+1, test_acc / (step+1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('klue/roberta-large', num_labels =6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN, tokenizer):\n",
    "\n",
    "    encoded_dict=tokenizer.encode_plus(\n",
    "    text = sent,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    truncation = True)\n",
    "\n",
    "    input_id=encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    #token_type_id = encoded_dict['token_type_ids']\n",
    "    token_type_id = 0\n",
    "\n",
    "    return input_id, attention_mask, token_type_id\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "outputs": [],
   "source": [
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
    "    }\n",
    "}\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = u'▁'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "        SentencePiece based tokenizer. Peculiarities:\n",
    "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
    "    \"\"\"\n",
    "    vocab_files_names = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            vocab_txt,\n",
    "            do_lower_case=False,\n",
    "            remove_space=True,\n",
    "            keep_accents=False,\n",
    "            unk_token=\"[UNK]\",\n",
    "            sep_token=\"[SEP]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            cls_token=\"[CLS]\",\n",
    "            mask_token=\"[MASK]\",\n",
    "            **kwargs):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        #self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
    "        #self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "        # 미리 학습한 sentencepiece model vocab file 사용 (위에서 정의)\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "    # getter setter 되도록 읽기 전용 필드\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "    #  setstate 로 들어간 것은 getstate 로 얻을 수 있다\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize('NFKD', outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
    "        \"\"\" Tokenize a string. \"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        if not sample:\n",
    "            pieces = self.sp_model.EncodeAsPieces(text)\n",
    "        else:\n",
    "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index, return_unicode=True):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A RoBERTa sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "outputs": [],
   "source": [
    "# 카카오 sentencepiece 카테고리분류기에서 활용 필요\n",
    "# tokenizer = KoBertTokenizer.from_pretrained('klue/roberta-large',  cache_dir='bert_ckpt', do_lower_case=False)\n",
    "# print('load kobert')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# input_id, attention_mask,_ =  bert_tokenizer(train_sent, MAX_LEN=MAX_LEN, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "outputs": [
    {
     "data": {
      "text/plain": "'소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 올리고 있으며 바다낚시터로도 유명하다. 항 주변에 설치된 양식장들은 섬사람들의 부지런한 생활상을 고스 란히 담고 있으며 일몰 때 섬의 정경은 바다의 아름다움을 그대로 품고 있는 듯하다. 또한 섬에는 각시여 전설 도둑바위 등의 설화가 전해 내려오고 있으며 매년 정월 풍어제 풍속이 이어지고 있다.'"
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(all_df['overview'].values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "outputs": [],
   "source": [
    "enc_ids = torch.tensor(enc.ids)\n",
    "attention_mask = torch.tensor(enc.attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [574]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1175\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1167\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1168\u001B[0m \u001B[38;5;124;03mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\u001B[39;00m\n\u001B[0;32m   1169\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\u001B[39;00m\n\u001B[0;32m   1170\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\u001B[39;00m\n\u001B[0;32m   1171\u001B[0m \u001B[38;5;124;03m    If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1173\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1175\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1176\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1178\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1181\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1182\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1183\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1185\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1186\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1187\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:783\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    782\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m--> 783\u001B[0m     batch_size, seq_length \u001B[38;5;241m=\u001B[39m input_shape\n\u001B[0;32m    784\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    785\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m inputs_embeds\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "model( enc_ids, attention_mask )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[0.8982,0.805,0.6393]])\n",
    "y = torch.LongTensor([3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.9173, grad_fn=<NllLossBackward0>)"
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [[tensor([25944]),\n   tensor([9051]),\n   tensor([8277]),\n   tensor([10773]),\n   tensor([6579]),\n   tensor([8992]),\n   tensor([3557]),\n   tensor([25529]),\n   tensor([15581]),\n   tensor([2050]),\n   tensor([11971]),\n   tensor([8348]),\n   tensor([6937]),\n   tensor([34801]),\n   tensor([12465]),\n   tensor([3020]),\n   tensor([3459]),\n   tensor([6926]),\n   tensor([9057]),\n   tensor([29689]),\n   tensor([6828]),\n   tensor([2736]),\n   tensor([21214]),\n   tensor([37501]),\n   tensor([3601]),\n   tensor([16646]),\n   tensor([9558]),\n   tensor([2386]),\n   tensor([3617]),\n   tensor([10193]),\n   tensor([6500]),\n   tensor([8289]),\n   tensor([2338]),\n   tensor([6532]),\n   tensor([19081]),\n   tensor([3567]),\n   tensor([32454]),\n   tensor([3625]),\n   tensor([7529]),\n   tensor([20661]),\n   tensor([6883]),\n   tensor([6625]),\n   tensor([22136]),\n   tensor([7298]),\n   tensor([20384]),\n   tensor([6500]),\n   tensor([7022]),\n   tensor([14805]),\n   tensor([39797]),\n   tensor([3682]),\n   tensor([26168]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0])]],\n 'attention_mask': [[tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([1]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0]),\n   tensor([0])]]}"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "data": {
      "text/plain": "512"
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([25944])"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['input_ids'][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'text', 'cat1', and 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [278]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mCateDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 3 required positional arguments: 'text', 'cat1', and 'tokenizer'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}