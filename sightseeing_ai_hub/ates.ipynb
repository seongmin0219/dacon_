{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# wandb.init(project=\"DACON_235978\", name=\"gnoeyheat\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"gnoeyheat\")\n",
    "parser.add_argument('--text_pretrained_model', default=\"roberta\", type=str)\n",
    "parser.add_argument('--text_len', default=300, type=int)\n",
    "parser.add_argument('--optimizer', default=\"sgd\", type=str) # sgd or adam\n",
    "parser.add_argument('--learning_rate', default=0.002, type=float)\n",
    "parser.add_argument('--loss', default='cc', type=str) # cc or fl\n",
    "parser.add_argument('--label_smoothing', default=0.1, type=float)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--epochs', default=30, type=int)\n",
    "parser.add_argument('--validation_split', default=0.1, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.config.update(args)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "text_pretrained_model = args.text_pretrained_model\n",
    "text_len = args.text_len\n",
    "BATCH_SIZE=args.batch_size\n",
    "EPOCHS=args.epochs\n",
    "VALIDATION_SPLIT=args.validation_split\n",
    "SEED=args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seeds()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "if args.text_pretrained_model == \"roberta\":\n",
    "    text_pretrained_model = \"klue/roberta-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_pretrained_model)\n",
    "# tokenizer.truncation_side = 'left'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "((16986,), (7280,), (16986,))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "X_txt = train[\"overview\"]\n",
    "X_test_txt = test[\"overview\"]\n",
    "\n",
    "y = train[\"cat3\"]\n",
    "y_encoder = {key : value for key, value in zip(np.unique(y), range(len(np.unique(y))))}\n",
    "y = np.array([y_encoder[k] for k in y])\n",
    "\n",
    "X_txt.shape, X_test_txt.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 올리고 있으며 바다낚시터로도 유명하다 항 주변에 설치된 양식장들은 섬사람들의 부지런한 생활상을 고스 란히 담고 있으며 일몰 때 섬의 정경은 바다의 아름다움을 그대로 품고 있는 듯하다 또한 섬에는 각시여 전설 도둑바위 등의 설화가 전해 내려오고 있으며 매년 정월 풍어제 풍속이 이어지고 있다'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_cleaning(df):\n",
    "    df = df.apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', ' ', x))\n",
    "    df = df.apply(lambda x : ' '.join(x.split()))\n",
    "    return df\n",
    "\n",
    "X_txt = text_cleaning(X_txt)\n",
    "X_test_txt = text_cleaning(X_test_txt)\n",
    "\n",
    "X_txt.iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(144.0, 144.0)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"len\"] = train[\"overview\"].apply(tokenizer.tokenize).apply(len)\n",
    "test[\"len\"] = test[\"overview\"].apply(tokenizer.tokenize).apply(len)\n",
    "train[\"len\"].median(), test[\"len\"].median()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence,\n",
    "        labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence = sentence\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.indexes = np.arange(len(self.sentence))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence = self.sentence[indexes]\n",
    "\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=text_len,\n",
    "            return_tensors=\"tf\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(SEED).shuffle(self.indexes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "((15287,), (1699,), (15287, 128), (1699, 128))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_txt, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_ds = DataGenerator(\n",
    "    X_train.values, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_ds = DataGenerator(\n",
    "    X_val.values, y_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "lr = 2e-2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x1c53bf90700>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x1c53bf90700>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"input_ids\"\n",
    ")\n",
    "attention_masks = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"attention_masks\"\n",
    ")\n",
    "token_type_ids = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"token_type_ids\"\n",
    ")\n",
    "\n",
    "bert_model = TFAutoModel.from_pretrained(text_pretrained_model, from_pt=True)\n",
    "bert_model.trainable = True\n",
    "\n",
    "bert_output = bert_model(\n",
    "    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    ")\n",
    "\n",
    "x = bert_output.last_hidden_state\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "output = layers.Dense(y_train.shape[1], activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  336656384  ['input_ids[0][0]',              \n",
      " el)                            thPooling(last_hidd               'attention_masks[0][0]',        \n",
      "                                en_state=(None, 300               'token_type_ids[0][0]']         \n",
      "                                , 1024),                                                          \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 1024)        0           ['tf_roberta_model[0][0]']       \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)           (None, 1024)         0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          131200      ['dropout_73[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 336,787,584\n",
      "Trainable params: 336,787,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if args.optimizer == \"sgd\":\n",
    "    optim = tf.keras.optimizers.SGD(\n",
    "        learning_rate=lr, momentum=0.9\n",
    "    )\n",
    "if args.loss == \"cc\":\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=args.label_smoothing\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=loss_function,\n",
    "    metrics=tfa.metrics.F1Score(num_classes=y_train.shape[1], average=\"weighted\")\n",
    ")\n",
    "# tf roberta_model 과 마지막 dense layer 에만 가중치가 존재할거고 그걸 학습해야 되지 않을까\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.losses.CategoricalCrossentropy at 0x1c514eed250>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# model checkpoint가 없어서 그런 듯한데 , 꼼꼼히 살펴보니까\n",
    "# checkpoint_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[16,300,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._5/output/dropout_17/dropout/random_uniform/RandomUniform\n (defined at c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\layers\\core\\dropout.py:105)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_50301]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/tf_roberta_model/roberta/encoder/layer_._5/output/dropout_17/dropout/random_uniform/RandomUniform:\nIn[0] model/tf_roberta_model/roberta/encoder/layer_._5/output/dropout_17/dropout/Shape:\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n>>>     res = shell.run_cell(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\DATA\\AppData\\Local\\Temp\\ipykernel_22756\\450307212.py\", line 20, in <cell line: 20>\n>>>     history = model.fit(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 746, in call\n>>>     outputs = self.roberta(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 558, in call\n>>>     encoder_outputs = self.encoder(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 414, in call\n>>>     for i, layer_module in enumerate(self.layer):\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 418, in call\n>>>     layer_outputs = layer_module(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 386, in call\n>>>     layer_output = self.bert_output(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 354, in call\n>>>     hidden_states = self.dropout(inputs=hidden_states, training=training)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 111, in call\n>>>     output = control_flow_util.smart_cond(training, dropped_inputs,\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 105, in smart_cond\n>>>     return tf.__internal__.smart_cond.smart_cond(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 105, in dropped_inputs\n>>>     return tf.nn.dropout(\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 20>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m lr \u001B[38;5;241m*\u001B[39m tf\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m      9\u001B[0m callback \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     10\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mLearningRateScheduler(scheduler),\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# tf.keras.callbacks.ModelCheckpoint(\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[0;32m     18\u001B[0m ]\n\u001B[1;32m---> 20\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 58\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     59\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     61\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m:  OOM when allocating tensor with shape[16,300,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._5/output/dropout_17/dropout/random_uniform/RandomUniform\n (defined at c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\layers\\core\\dropout.py:105)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_50301]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/tf_roberta_model/roberta/encoder/layer_._5/output/dropout_17/dropout/random_uniform/RandomUniform:\nIn[0] model/tf_roberta_model/roberta/encoder/layer_._5/output/dropout_17/dropout/Shape:\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n>>>     res = shell.run_cell(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\DATA\\AppData\\Local\\Temp\\ipykernel_22756\\450307212.py\", line 20, in <cell line: 20>\n>>>     history = model.fit(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 746, in call\n>>>     outputs = self.roberta(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 558, in call\n>>>     encoder_outputs = self.encoder(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 414, in call\n>>>     for i, layer_module in enumerate(self.layer):\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 418, in call\n>>>     layer_outputs = layer_module(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 386, in call\n>>>     layer_output = self.bert_output(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 354, in call\n>>>     hidden_states = self.dropout(inputs=hidden_states, training=training)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 111, in call\n>>>     output = control_flow_util.smart_cond(training, dropped_inputs,\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 105, in smart_cond\n>>>     return tf.__internal__.smart_cond.smart_cond(\n>>> \n>>>   File \"c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\layers\\core\\dropout.py\", line 105, in dropped_inputs\n>>>     return tf.nn.dropout(\n>>> "
     ]
    }
   ],
   "source": [
    "checkpoint_path = f\"load_model/{parser.description}\"\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback = [\n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "    # tf.keras.callbacks.ModelCheckpoint(\n",
    "    #     checkpoint_path,\n",
    "    #     monitor=\"val_f1_score\",\n",
    "    #     save_best_only=True,\n",
    "    #     save_weights_only=True,\n",
    "    #     mode=\"max\",\n",
    "    # )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=2,\n",
    "    callbacks=[callback],\n",
    "    validation_data=val_ds,\n",
    ")\n",
    "# 그러기엔 loss가 높다\n",
    "\n",
    "# train 이 안되는 현상 나랑 똑같다 weight가 reshape 되면 그렇다고 ?\n",
    "# https://stackoverflow.com/questions/61763029/tensorflow-tf-reshape-causes-gradients-do-not-exist-for-variables\n",
    "# eager execution 과 관련이 있다..?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['f1_score']\n",
    "val_acc = history.history['val_f1_score']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(acc, label='Training Weighted-F1')\n",
    "plt.plot(val_acc, label='Validation Weighted-F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Weighted-F1')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_weighted_f1 = model.evaluate(val_ds)[1]\n",
    "print(f\"val_weighted_f1: {val_weighted_f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_ds = DataGenerator(\n",
    "    X_test_txt.values, None,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    include_targets=False,\n",
    ")\n",
    "\n",
    "pred_prob = []\n",
    "for i in range(test_ds.__len__()):\n",
    "    pred_prob.append(model.predict(test_ds.__getitem__(i)))\n",
    "pred_prob = np.vstack(pred_prob)\n",
    "pred = np.argmax(pred_prob, axis=1)\n",
    "\n",
    "y_decoder = {value : key for key, value in y_encoder.items()}\n",
    "result = np.array([y_decoder[v] for v in pred])\n",
    "\n",
    "pd.Series(result).value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "submission[\"cat3\"] = result\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}