{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Make Fold\n",
    "\n",
    "제가 데이터를 다운로드 하여 놓은 경로는 \"/workspace/Dacon/\" 입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# 잘되지도 않았고 memory error 때문에 못햇다 결국 비슷한 코드였는데 , 그리고 그건 예상밖으로(어쩌면 안으로) 큰 차이\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./input/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat3'].values)\n",
    "df['cat3'] = le.transform(df['cat3'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['5일장', 'ATV', 'MTB', '강', '게스트하우스', '계곡', '고궁', '고택', '골프', '공연장',\n       '공예,공방', '공원', '관광단지', '국립공원', '군립공원', '기념관', '기념탑/기념비/전망대',\n       '기암괴석', '기타', '기타행사', '농.산.어촌 체험', '다리/대교', '대중콘서트', '대형서점',\n       '도립공원', '도서관', '동굴', '동상', '등대', '래프팅', '면세점', '모텔', '문', '문화관광축제',\n       '문화원', '문화전수시설', '뮤지컬', '미술관/화랑', '민물낚시', '민박', '민속마을', '바/까페',\n       '바다낚시', '박람회', '박물관', '발전소', '백화점', '번지점프', '복합 레포츠', '분수', '빙벽등반',\n       '사격장', '사찰', '산', '상설시장', '생가', '서비스드레지던스', '서양식', '섬', '성',\n       '수련시설', '수목원', '수상레포츠', '수영', '스노쿨링/스킨스쿠버다이빙', '스카이다이빙', '스케이트',\n       '스키(보드) 렌탈샵', '스키/스노보드', '승마', '식음료', '썰매장', '안보관광', '야영장,오토캠핑장',\n       '약수터', '연극', '영화관', '온천/욕장/스파', '외국문화원', '요트', '윈드서핑/제트스키',\n       '유람선/잠수함관광', '유명건물', '유스호스텔', '유원지', '유적지/사적지', '이색거리', '이색찜질방',\n       '이색체험', '인라인(실내 인라인 포함)', '일반축제', '일식', '자동차경주', '자연생태관광지',\n       '자연휴양림', '자전거하이킹', '전문상가', '전시관', '전통공연', '종교성지', '중식', '채식전문점',\n       '카약/카누', '카지노', '카트', '컨벤션', '컨벤션센터', '콘도미니엄', '클래식음악회', '클럽',\n       '터널', '테마공원', '트래킹', '특산물판매점', '패밀리레스토랑', '펜션', '폭포', '학교', '한식',\n       '한옥스테이', '항구/포구', '해수욕장', '해안절경', '헬스투어', '헹글라이딩/패러글라이딩', '호수',\n       '홈스테이', '희귀동.식물'], dtype=object)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat2'].values)\n",
    "df['cat2'] = le.transform(df['cat2'].values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat1'].values)\n",
    "df['cat1'] = le.transform(df['cat1'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "df['kfold'] = -1\n",
    "for i in range(5):\n",
    "    df_idx, valid_idx = list(folds.split(df.values, df['cat3']))[i]\n",
    "    valid = df.iloc[valid_idx]\n",
    "\n",
    "    df.loc[df[df.id.isin(valid.id) == True].index.to_list(), 'kfold'] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df.to_csv('./input/train_folds.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                id                       img_path  \\\n0      TRAIN_00000  ./image/train/TRAIN_00000.jpg   \n1      TRAIN_00001  ./image/train/TRAIN_00001.jpg   \n2      TRAIN_00002  ./image/train/TRAIN_00002.jpg   \n3      TRAIN_00003  ./image/train/TRAIN_00003.jpg   \n4      TRAIN_00004  ./image/train/TRAIN_00004.jpg   \n...            ...                            ...   \n16981  TRAIN_16981  ./image/train/TRAIN_16981.jpg   \n16982  TRAIN_16982  ./image/train/TRAIN_16982.jpg   \n16983  TRAIN_16983  ./image/train/TRAIN_16983.jpg   \n16984  TRAIN_16984  ./image/train/TRAIN_16984.jpg   \n16985  TRAIN_16985  ./image/train/TRAIN_16985.jpg   \n\n                                                overview  cat1  cat2  cat3  \\\n0      소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 ...     5    13   120   \n1      경기도 이천시 모가면에 있는 골프장으로 대중제 18홀이다. 회원제로 개장을 했다가 ...     0    11     8   \n2      금오산성숯불갈비는 한우고기만을 전문적으로 취급하고 사용하는 부식 자재 또한 유기농법...     3    12   118   \n3      철판 위에서 요리하는 안동찜닭을 맛볼 수 있는 곳이다. 경상북도 안동시에 있는 한식...     3    12   118   \n4      ※ 영업시간 10:30 ~ 20:30\\n\\n3대에 걸쳐 아귀만을 전문으로 취급하는 ...     3    12   118   \n...                                                  ...   ...   ...   ...   \n16981  해발 12000m에 자리한 식담겸 카페점문점이다.<br>곤드레밥과 감자전을 판매하고...     3    12   118   \n16982  설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...     2     9    31   \n16983  충남 서산시 중심가에 위치한 줌모텔은 프라이버스가 보장되는 조용한 공간으로 가치가 ...     2     9    31   \n16984  토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...     0    11    73   \n16985  포천의 진산으로 불리우는 왕방산(王訪山)에는 천년의 역사를 간직하고 있는 왕산사(王...     4    10    52   \n\n       kfold  \n0          4  \n1          4  \n2          4  \n3          3  \n4          1  \n...      ...  \n16981      0  \n16982      1  \n16983      4  \n16984      1  \n16985      3  \n\n[16986 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>img_path</th>\n      <th>overview</th>\n      <th>cat1</th>\n      <th>cat2</th>\n      <th>cat3</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_00000</td>\n      <td>./image/train/TRAIN_00000.jpg</td>\n      <td>소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 ...</td>\n      <td>5</td>\n      <td>13</td>\n      <td>120</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_00001</td>\n      <td>./image/train/TRAIN_00001.jpg</td>\n      <td>경기도 이천시 모가면에 있는 골프장으로 대중제 18홀이다. 회원제로 개장을 했다가 ...</td>\n      <td>0</td>\n      <td>11</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_00002</td>\n      <td>./image/train/TRAIN_00002.jpg</td>\n      <td>금오산성숯불갈비는 한우고기만을 전문적으로 취급하고 사용하는 부식 자재 또한 유기농법...</td>\n      <td>3</td>\n      <td>12</td>\n      <td>118</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_00003</td>\n      <td>./image/train/TRAIN_00003.jpg</td>\n      <td>철판 위에서 요리하는 안동찜닭을 맛볼 수 있는 곳이다. 경상북도 안동시에 있는 한식...</td>\n      <td>3</td>\n      <td>12</td>\n      <td>118</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_00004</td>\n      <td>./image/train/TRAIN_00004.jpg</td>\n      <td>※ 영업시간 10:30 ~ 20:30\\n\\n3대에 걸쳐 아귀만을 전문으로 취급하는 ...</td>\n      <td>3</td>\n      <td>12</td>\n      <td>118</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16981</th>\n      <td>TRAIN_16981</td>\n      <td>./image/train/TRAIN_16981.jpg</td>\n      <td>해발 12000m에 자리한 식담겸 카페점문점이다.&lt;br&gt;곤드레밥과 감자전을 판매하고...</td>\n      <td>3</td>\n      <td>12</td>\n      <td>118</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16982</th>\n      <td>TRAIN_16982</td>\n      <td>./image/train/TRAIN_16982.jpg</td>\n      <td>설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...</td>\n      <td>2</td>\n      <td>9</td>\n      <td>31</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16983</th>\n      <td>TRAIN_16983</td>\n      <td>./image/train/TRAIN_16983.jpg</td>\n      <td>충남 서산시 중심가에 위치한 줌모텔은 프라이버스가 보장되는 조용한 공간으로 가치가 ...</td>\n      <td>2</td>\n      <td>9</td>\n      <td>31</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16984</th>\n      <td>TRAIN_16984</td>\n      <td>./image/train/TRAIN_16984.jpg</td>\n      <td>토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...</td>\n      <td>0</td>\n      <td>11</td>\n      <td>73</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16985</th>\n      <td>TRAIN_16985</td>\n      <td>./image/train/TRAIN_16985.jpg</td>\n      <td>포천의 진산으로 불리우는 왕방산(王訪山)에는 천년의 역사를 간직하고 있는 왕산사(王...</td>\n      <td>4</td>\n      <td>10</td>\n      <td>52</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>16986 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "df = pd.read_csv('./input/train_folds.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CategoryDataset(Dataset):\n",
    "  def __init__(self, text,  cats3, tokenizer,  max_len):\n",
    "    self.text = text\n",
    "    # cat1,cat2를 없앤 코드 생각해보니 안없애도됐다\n",
    "    self.cats3 = cats3\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, item):\n",
    "\n",
    "    text = str(self.text[item])\n",
    "\n",
    "\n",
    "    cat3 = self.cats3[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'cats3': torch.tensor(cat3, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "def create_data_loader(df, tokenizer,  max_len, batch_size, shuffle_=False):\n",
    "    ds = CategoryDataset(\n",
    "        text=df.overview.to_numpy(),\n",
    "        cats3=df.cat3.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return \\\n",
    "        DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle = shuffle_\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "from transformers import AutoModel,ViTModel,ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "\n",
    "class TourClassifier(nn.Module):\n",
    "  def __init__(self,  text_model_name ,n_classes3=128):\n",
    "    super(TourClassifier, self).__init__()\n",
    "    self.text_model = AutoModel.from_pretrained(text_model_name).to(device)\n",
    "    # gradient_checkpointing_enabel() 안함\n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def get_cls(target_size):\n",
    "      return nn.Sequential(\n",
    "          nn.Linear(self.text_model.config.hidden_size, self.text_model.config.hidden_size),\n",
    "          nn.LayerNorm(self.text_model.config.hidden_size),\n",
    "          nn.Dropout(p = 0.1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(self.text_model.config.hidden_size, target_size),\n",
    "      )\n",
    "\n",
    "    self.cls3 = get_cls(n_classes3)\n",
    "\n",
    "\n",
    "  def forward(self, input_ids, attention_mask ):\n",
    "    text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # outputs = transformer_encoder(concat_outputs)\n",
    "    # cls token\n",
    "    # outputs = outputs[:,0]\n",
    "    # output = self.drop(outputs)\n",
    "    #\n",
    "    # out1 = self.cls(output)\n",
    "    # out2 = self.cls2(output)\n",
    "\n",
    "\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=self.text_model.config.hidden_size, nhead=8).to(device)\n",
    "    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2).to(device)\n",
    "    outputs = transformer_encoder(text_output.last_hidden_state)\n",
    "    #cls token\n",
    "    print('output shape after pass transforeer_encoder  : {outputs}'.format(outputs=outputs.shape))\n",
    "\n",
    "    outputs = outputs[:,0]\n",
    "    print('output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : {outputs}'.format(outputs=outputs.shape))\n",
    "    output = self.drop(outputs)\n",
    "    print('output shape after pass transforeer_encoder  : {outputs}'.format(outputs=outputs.shape))\n",
    "\n",
    "    out3 = self.cls3(output)\n",
    "    print('output shape final  : {outputs}'.format(outputs=out3.shape))\n",
    "\n",
    "    return out3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def calc_tour_acc(pred, label):\n",
    "    _, idx = pred.max(1)\n",
    "    acc = torch.eq(idx, label).sum().item() / idx.size()[0]\n",
    "    x = label.cpu().numpy()\n",
    "    y = idx.cpu().numpy()\n",
    "    f1_acc = f1_score(x, y, average='weighted')\n",
    "    return acc,f1_acc\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import argparse\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples,epoch):\n",
    "\n",
    "  batch_time = AverageMeter()\n",
    "  data_time = AverageMeter()\n",
    "  losses = AverageMeter()\n",
    "  accuracies = AverageMeter()\n",
    "  f1_accuracies = AverageMeter()\n",
    "  sent_count = AverageMeter()\n",
    "  start = end = time.time()\n",
    "\n",
    "  model = model.train()\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for step,d in enumerate(data_loader):\n",
    "    print(step)\n",
    "    data_time.update(time.time() - end)\n",
    "    batch_size = d[\"input_ids\"].size(0)\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    print(input_ids.shape, attention_mask.shape)\n",
    "    cats3 = d[\"cats3\"].to(device)\n",
    "\n",
    "    outputs  = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    _, preds = torch.max(outputs , dim=1)\n",
    "\n",
    "    loss = loss_fn(outputs, cats3)\n",
    "\n",
    "\n",
    "    correct_predictions += torch.sum(preds == cats3)\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    # 모델 매개 변수에 대한 변화도 수집\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    # 변화도에 따라 업데이트 매개변수 조정\n",
    "    optimizer.step()\n",
    "    # scheduler의 의미: Learning Rate Scheduler => learning rate를 조절한다.\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    sent_count.update(batch_size)\n",
    "    if step % 3 == 0 or step == (len(data_loader)-1):\n",
    "                # 디버깅 해보니 acc가 왜 이렇게 나왔지\n",
    "                acc,f1_acc = calc_tour_acc(outputs, cats3)\n",
    "                accuracies.update(acc, batch_size)\n",
    "                f1_accuracies.update(f1_acc, batch_size)\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.3f}({loss.avg:.3f}) '\n",
    "                      'Acc: {acc.val:.3f}({acc.avg:.3f}) '\n",
    "                      'f1_Acc: {f1_acc.val:.3f}({f1_acc.avg:.3f}) '\n",
    "                      'sent/s {sent_s:.0f} '\n",
    "                      .format(\n",
    "                      epoch, step+1, len(data_loader),\n",
    "                      data_time=data_time, loss=losses,\n",
    "                      acc=accuracies,\n",
    "                      f1_acc=f1_accuracies,\n",
    "                      remain=timeSince(start, float(step+1)/len(data_loader)),\n",
    "                      sent_s=sent_count.avg/batch_time.avg\n",
    "                      ))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, losses.avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def validate(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  cnt = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    with torch.no_grad():\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      cats3 = d[\"cats3\"].to(device)\n",
    "      outputs,outputs2,outputs3 = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs3, dim=1)\n",
    "      loss = loss_fn(outputs3, cats3)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == cats3)\n",
    "      losses.append(loss.item())\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "      if cnt == 0:\n",
    "        cnt +=1\n",
    "        outputs3_arr = outputs3\n",
    "        cats3_arr = cats3\n",
    "      else:\n",
    "        outputs3_arr = torch.cat([outputs3_arr, outputs3],0)\n",
    "        cats3_arr = torch.cat([cats3_arr, cats3],0)\n",
    "  acc,f1_acc = calc_tour_acc(outputs3_arr, cats3_arr)\n",
    "  return f1_acc, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# outputs,outputs2,outputs3 = model(\n",
    "#         input_ids=input_ids,\n",
    "#         attention_mask=attention_mask,\n",
    "#         pixel_values=pixel_values\n",
    "#       )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# train.cat3.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "train = df[df[\"kfold\"] != 0].reset_index(drop=True)\n",
    "valid = df[df[\"kfold\"] == 0].reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "#feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch32-384')\n",
    "# train_data_loader = create_data_loader(train, tokenizer, feature_extractor, 256, 16, shuffle_=True)\n",
    "train_data_loader = create_data_loader(train, tokenizer,  256, 8, shuffle_=True)\n",
    "# valid_data_loader = create_data_loader(valid, tokenizer, feature_extractor, 256, 16)\n",
    "valid_data_loader = create_data_loader(valid, tokenizer,   256, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cat1 , 2 뺀거말고는 다 같다\n",
    "EPOCHS = 30\n",
    "model = TourClassifier( text_model_name = \"klue/roberta-large\" ).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr= 3e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "optimizer,\n",
    "num_warmup_steps=int(total_steps*0.1),\n",
    "num_training_steps=total_steps\n",
    ")\n",
    "#Cross Entropy Loss은 머신 러닝 분류 모델의 발견된 확률 분포와 예측 분포 사이의 차이를 측정합니다.\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/29\n",
      "----------\n",
      "0\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 256, 1024])\n",
      "output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : torch.Size([8, 1024])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 1024])\n",
      "output shape final  : torch.Size([8, 128])\n",
      "Epoch: [0][1/1699] Data 0.005 (0.005) Elapsed 0m 14s (remain 420m 22s) Loss: 4.603(4.603) Acc: 0.000(0.000) f1_Acc: 0.000(0.000) sent/s 1 \n",
      "1\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 256, 1024])\n",
      "output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : torch.Size([8, 1024])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 1024])\n",
      "output shape final  : torch.Size([8, 128])\n",
      "2\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 256, 1024])\n",
      "output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : torch.Size([8, 1024])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 1024])\n",
      "output shape final  : torch.Size([8, 128])\n",
      "3\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 256, 1024])\n",
      "output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : torch.Size([8, 1024])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 1024])\n",
      "output shape final  : torch.Size([8, 128])\n",
      "Epoch: [0][4/1699] Data 0.004 (0.004) Elapsed 0m 59s (remain 423m 41s) Loss: 4.843(4.725) Acc: 0.000(0.000) f1_Acc: 0.000(0.000) sent/s 1 \n",
      "4\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 256, 1024])\n",
      "output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : torch.Size([8, 1024])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 1024])\n",
      "output shape final  : torch.Size([8, 128])\n",
      "5\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 256, 1024])\n",
      "output shape after pass transforeer_encoder and choice only use to [CLS] TOKEN : torch.Size([8, 1024])\n",
      "output shape after pass transforeer_encoder  : torch.Size([8, 1024])\n",
      "output shape final  : torch.Size([8, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [67]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m train_acc, train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepoch\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m validate_acc, validate_loss \u001B[38;5;241m=\u001B[39m validate(\n\u001B[0;32m     17\u001B[0m     model,\n\u001B[0;32m     18\u001B[0m     valid_data_loader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28mlen\u001B[39m(valid)\n\u001B[0;32m     24\u001B[0m )\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validate_acc \u001B[38;5;241m>\u001B[39m max_acc:\n",
      "Input \u001B[1;32mIn [49]\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples, epoch)\u001B[0m\n\u001B[0;32m     43\u001B[0m losses\u001B[38;5;241m.\u001B[39mupdate(loss\u001B[38;5;241m.\u001B[39mitem(), batch_size)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# 모델 매개 변수에 대한 변화도 수집\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m nn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# 변화도에 따라 업데이트 매개변수 조정\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\_tensor.py:307\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    300\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    301\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    305\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    306\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 307\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    152\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m--> 154\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "max_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print('-' * 10)\n",
    "    print(f'Epoch {epoch}/{EPOCHS-1}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train),\n",
    "        epoch\n",
    "    )\n",
    "    validate_acc, validate_loss = validate(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(valid)\n",
    "    )\n",
    "\n",
    "    if validate_acc > max_acc:\n",
    "        max_acc = validate_acc\n",
    "        torch.save(model.state_dict(),f'tourbaseline_fold0.pt')\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    print(f'Validate loss {validate_loss} accuracy {validate_acc}')\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    # bert의 output dim 은 transformer encoder 를 통해 변환시켜서 label과 계산을 할 수 있도록 변경한다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CategoryDataset(Dataset):\n",
    "  def __init__(self, text,  tokenizer,  max_len):\n",
    "    self.text = text\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.text[item])\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten()\n",
    "    }\n",
    "\n",
    "def create_data_loader(df, tokenizer,  max_len, batch_size, shuffle_=False):\n",
    "    ds = CategoryDataset(\n",
    "        text=df.overview.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle = shuffle_\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def inference(model,data_loader,device,n_examples):\n",
    "  model = model.eval()\n",
    "  preds_arr = []\n",
    "  preds_arr2 = []\n",
    "  preds_arr3 = []\n",
    "  for d in tqdm(data_loader):\n",
    "    with torch.no_grad():\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "\n",
    "      outputs,outputs2,outputs3 = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      _, preds2 = torch.max(outputs2, dim=1)\n",
    "      _, preds3 = torch.max(outputs3, dim=1)\n",
    "\n",
    "      preds_arr.append(preds.cpu().numpy())\n",
    "      preds_arr2.append(preds2.cpu().numpy())\n",
    "      preds_arr3.append(preds3.cpu().numpy())\n",
    "\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "  return preds_arr, preds_arr2, preds_arr3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = pd.read_csv('./input/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_data_loader = create_data_loader(test, tokenizer,  256, 1)\n",
    "\n",
    "preds_arr, preds_arr2, preds_arr3 = inference(\n",
    "        model,\n",
    "        eval_data_loader,\n",
    "        device,\n",
    "        len(test)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./input/sample_submission.csv')\n",
    "arr = ['5일장', 'ATV', 'MTB', '강', '게스트하우스', '계곡', '고궁', '고택', '골프', '공연장',\n",
    "       '공예,공방', '공원', '관광단지', '국립공원', '군립공원', '기념관', '기념탑/기념비/전망대',\n",
    "       '기암괴석', '기타', '기타행사', '농.산.어촌 체험', '다리/대교', '대중콘서트', '대형서점',\n",
    "       '도립공원', '도서관', '동굴', '동상', '등대', '래프팅', '면세점', '모텔', '문', '문화관광축제',\n",
    "       '문화원', '문화전수시설', '뮤지컬', '미술관/화랑', '민물낚시', '민박', '민속마을', '바/까페',\n",
    "       '바다낚시', '박람회', '박물관', '발전소', '백화점', '번지점프', '복합 레포츠', '분수', '빙벽등반',\n",
    "       '사격장', '사찰', '산', '상설시장', '생가', '서비스드레지던스', '서양식', '섬', '성',\n",
    "       '수련시설', '수목원', '수상레포츠', '수영', '스노쿨링/스킨스쿠버다이빙', '스카이다이빙', '스케이트',\n",
    "       '스키(보드) 렌탈샵', '스키/스노보드', '승마', '식음료', '썰매장', '안보관광', '야영장,오토캠핑장',\n",
    "       '약수터', '연극', '영화관', '온천/욕장/스파', '외국문화원', '요트', '윈드서핑/제트스키',\n",
    "       '유람선/잠수함관광', '유명건물', '유스호스텔', '유원지', '유적지/사적지', '이색거리', '이색찜질방',\n",
    "       '이색체험', '인라인(실내 인라인 포함)', '일반축제', '일식', '자동차경주', '자연생태관광지',\n",
    "       '자연휴양림', '자전거하이킹', '전문상가', '전시관', '전통공연', '종교성지', '중식', '채식전문점',\n",
    "       '카약/카누', '카지노', '카트', '컨벤션', '컨벤션센터', '콘도미니엄', '클래식음악회', '클럽',\n",
    "       '터널', '테마공원', '트래킹', '특산물판매점', '패밀리레스토랑', '펜션', '폭포', '학교', '한식',\n",
    "       '한옥스테이', '항구/포구', '해수욕장', '해안절경', '헬스투어', '헹글라이딩/패러글라이딩', '호수',\n",
    "       '홈스테이', '희귀동.식물']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(preds_arr3)):\n",
    "    sample_submission.loc[i,'cat3'] = arr[preds_arr3[i][0]]\n",
    "\n",
    "sample_submission.to_csv('baseline.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}