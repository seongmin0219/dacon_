{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./input/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                overview   cat3\n11266  1971년 국가 어항으로 지정된 방어진항은 넓은 부지의 수협위판장과 경매장이 있는 ...  항구/포구\n1248   격포항은 해양수산부에서 선정한 우리나라 '아름다운 어촌 100개소' 중 한곳으로, ...  항구/포구\n6719   안면대교를 건너 만나는 첫 삼거리에서 오른쪽으로 조금 들어가면 나온다. 포구에는 횟...  항구/포구\n1671   왜목마을은 충남 당진시 서해의 최북단에 있는 마을이다. 지형적으로 바다를 동서로 양...  항구/포구\n7480   보성군 회천면 율포리에 있는 율포항은 1972년 지방 어항으로 지정되었고, 태풍 내...  항구/포구",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overview</th>\n      <th>cat3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11266</th>\n      <td>1971년 국가 어항으로 지정된 방어진항은 넓은 부지의 수협위판장과 경매장이 있는 ...</td>\n      <td>항구/포구</td>\n    </tr>\n    <tr>\n      <th>1248</th>\n      <td>격포항은 해양수산부에서 선정한 우리나라 '아름다운 어촌 100개소' 중 한곳으로, ...</td>\n      <td>항구/포구</td>\n    </tr>\n    <tr>\n      <th>6719</th>\n      <td>안면대교를 건너 만나는 첫 삼거리에서 오른쪽으로 조금 들어가면 나온다. 포구에는 횟...</td>\n      <td>항구/포구</td>\n    </tr>\n    <tr>\n      <th>1671</th>\n      <td>왜목마을은 충남 당진시 서해의 최북단에 있는 마을이다. 지형적으로 바다를 동서로 양...</td>\n      <td>항구/포구</td>\n    </tr>\n    <tr>\n      <th>7480</th>\n      <td>보성군 회천면 율포리에 있는 율포항은 1972년 지방 어항으로 지정되었고, 태풍 내...</td>\n      <td>항구/포구</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.cat3 =='항구/포구'].sample(5)[['overview','cat3']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sentences = df.overview.values\n",
    "labels = df['cat3'].astype('category').cat.codes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,RobertaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "\n",
    "# RobertTokenizer.from_pretrained('klue/robert-large')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='monologg/kobert', vocab_size=8002, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 올리고 있으며 바다낚시터로도 유명하다. 항 주변에 설치된 양식장들은 섬사람들의 부지런한 생활상을 고스 란히 담고 있으며 일몰 때 섬의 정경은 바다의 아름다움을 그대로 품고 있는 듯하다. 또한, 섬에는 각시여 전설, 도둑바위 등의 설화가 전해 내려오고 있으며, 매년 정월 풍어제 풍속이 이어지고 있다.<br>\n",
      "Tokenized:  ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '김', '[UNK]', '해서', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.', '항', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '때', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '있는', '[UNK]', '.', '[UNK]', ',', '[UNK]', '[UNK]', '[UNK]', ',', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ',', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '있다', '.', '<', '[UNK]', '>']\n",
      "Token IDs:  [0, 0, 0, 0, 0, 0, 5586, 0, 7850, 0, 0, 0, 0, 0, 0, 54, 7846, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5965, 0, 0, 0, 0, 0, 0, 7142, 0, 54, 0, 46, 0, 0, 0, 46, 0, 0, 0, 0, 0, 0, 46, 0, 0, 0, 0, 0, 7143, 54, 253, 0, 257]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kobert'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkobert\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m download, get_tokenizer\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'kobert'"
     ]
    }
   ],
   "source": [
    "from kobert.utils.utils import download, get_tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# tokenizer에 따라 UNK가 너무많다 확실히\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m pt \u001B[38;5;241m=\u001B[39m \u001B[43margs\u001B[49m\u001B[38;5;241m.\u001B[39mpt\n\u001B[0;32m      5\u001B[0m tokenizer_path \u001B[38;5;241m=\u001B[39m get_tokenizer()\n\u001B[0;32m      6\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m nlp\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mBERTSPTokenizer(tokenizer_path,vocab,lower\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenizer에 따라 UNK가 너무많다 확실히\n",
    "\n",
    "\n",
    "tokenizer_path = get_tokenizer()\n",
    "tokenizer = nlp.data.BERTSPTokenizer(tokenizer_path,vocab,lower=False)\n",
    "#tokenizer = KoBertTokenizer.from_pretrained(pt,  cache_dir='bert_ckpt', do_lower_case=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 올리고 있으며 바다낚시터로도 유명하다. 항 주변에 설치된 양식장들은 섬사람들의 부지런한 생활상을 고스 란히 담고 있으며 일몰 때 섬의 정경은 바다의 아름다움을 그대로 품고 있는 듯하다. 또한, 섬에는 각시여 전설, 도둑바위 등의 설화가 전해 내려오고 있으며, 매년 정월 풍어제 풍속이 이어지고 있다.<br>\n",
      "Tokenized:  ['ᄉ', '##ᅩ', '##ᄋ', '##ᅡ', '##ᆫ', '##ᄒ', '##ᅡ', '##ᆼ', '##ᄋ', '##ᅳ', '##ᆫ', 'ᄌ', '##ᅩ', '##ᄋ', '##ᅭ', '##ᆼ', '##ᄒ', '##ᅡ', '##ᆫ', 'ᄉ', '##ᅥ', '##ᆷ', '##ᄋ', '##ᅳ', '##ᄅ', '##ᅩ', 'ᄋ', '##ᅵ', '##ᆫ', '##ᄀ', '##ᅳ', '##ᆫ', '##ᄒ', '##ᅢ', '##ᄋ', '##ᅡ', '##ᆫ', '##ᄋ', '##ᅵ', 'ᄎ', '##ᅥ', '##ᆼ', '##ᄌ', '##ᅥ', '##ᆼ', '##ᄒ', '##ᅢ', '##ᄋ', '##ᅧ', '##ᆨ', '##ᄋ', '##ᅳ', '##ᄅ', '##ᅩ', '[UNK]', 'ᄀ', '##ᅵ', '##ᆷ', '[UNK]', 'ᄒ', '##ᅢ', '##ᄉ', '##ᅥ', '[UNK]', 'ᄉ', '##ᅩ', '##ᄃ', '##ᅳ', '##ᆨ', '##ᄋ', '##ᅳ', '##ᆯ', 'ᄋ', '##ᅩ', '##ᆯ', '##ᄅ', '##ᅵ', '##ᄀ', '##ᅩ', '[UNK]', '[UNK]', 'ᄋ', '##ᅲ', '##ᄆ', '##ᅧ', '##ᆼ', '##ᄒ', '##ᅡ', '##ᄃ', '##ᅡ', '.', 'ᄒ', '##ᅡ', '##ᆼ', 'ᄌ', '##ᅮ', '##ᄇ', '##ᅧ', '##ᆫ', '##ᄋ', '##ᅦ', '[UNK]', '[UNK]', 'ᄉ', '##ᅥ', '##ᆷ', '##ᄉ', '##ᅡ', '##ᄅ', '##ᅡ', '##ᆷ', '##ᄃ', '##ᅳ', '##ᆯ', '##ᄋ', '##ᅴ', 'ᄇ', '##ᅮ', '##ᄌ', '##ᅵ', '##ᄅ', '##ᅥ', '##ᆫ', '##ᄒ', '##ᅡ', '##ᆫ', 'ᄉ', '##ᅢ', '##ᆼ', '##ᄒ', '##ᅪ', '##ᆯ', '##ᄉ', '##ᅡ', '##ᆼ', '##ᄋ', '##ᅳ', '##ᆯ', 'ᄀ', '##ᅩ', '##ᄉ', '##ᅳ', 'ᄅ', '##ᅡ', '##ᆫ', '##ᄒ', '##ᅵ', 'ᄃ', '##ᅡ', '##ᆷ', '##ᄀ', '##ᅩ', '[UNK]', 'ᄋ', '##ᅵ', '##ᆯ', '##ᄆ', '##ᅩ', '##ᆯ', '[UNK]', 'ᄉ', '##ᅥ', '##ᆷ', '##ᄋ', '##ᅴ', 'ᄌ', '##ᅥ', '##ᆼ', '##ᄀ', '##ᅧ', '##ᆼ', '##ᄋ', '##ᅳ', '##ᆫ', 'ᄇ', '##ᅡ', '##ᄃ', '##ᅡ', '##ᄋ', '##ᅴ', 'ᄋ', '##ᅡ', '##ᄅ', '##ᅳ', '##ᆷ', '##ᄃ', '##ᅡ', '##ᄋ', '##ᅮ', '##ᆷ', '##ᄋ', '##ᅳ', '##ᆯ', 'ᄀ', '##ᅳ', '##ᄃ', '##ᅢ', '##ᄅ', '##ᅩ', 'ᄑ', '##ᅮ', '##ᆷ', '##ᄀ', '##ᅩ', '[UNK]', '[UNK]', '.', '[UNK]', ',', 'ᄉ', '##ᅥ', '##ᆷ', '##ᄋ', '##ᅦ', '##ᄂ', '##ᅳ', '##ᆫ', 'ᄀ', '##ᅡ', '##ᆨ', '##ᄉ', '##ᅵ', '##ᄋ', '##ᅧ', 'ᄌ', '##ᅥ', '##ᆫ', '##ᄉ', '##ᅥ', '##ᆯ', ',', '[UNK]', 'ᄃ', '##ᅳ', '##ᆼ', '##ᄋ', '##ᅴ', 'ᄉ', '##ᅥ', '##ᆯ', '##ᄒ', '##ᅪ', '##ᄀ', '##ᅡ', 'ᄌ', '##ᅥ', '##ᆫ', '##ᄒ', '##ᅢ', 'ᄂ', '##ᅢ', '##ᄅ', '##ᅧ', '##ᄋ', '##ᅩ', '##ᄀ', '##ᅩ', '[UNK]', ',', 'ᄆ', '##ᅢ', '##ᄂ', '##ᅧ', '##ᆫ', 'ᄌ', '##ᅥ', '##ᆼ', '##ᄋ', '##ᅯ', '##ᆯ', 'ᄑ', '##ᅮ', '##ᆼ', '##ᄋ', '##ᅥ', '##ᄌ', '##ᅦ', 'ᄑ', '##ᅮ', '##ᆼ', '##ᄉ', '##ᅩ', '##ᆨ', '##ᄋ', '##ᅵ', 'ᄋ', '##ᅵ', '##ᄋ', '##ᅥ', '##ᄌ', '##ᅵ', '##ᄀ', '##ᅩ', '[UNK]', '.', '<', 'br', '>']\n",
      "Token IDs:  [1461, 30011, 29999, 30006, 30021, 30005, 30006, 30025, 29999, 30017, 30021, 1464, 30011, 29999, 30013, 30025, 30005, 30006, 30021, 1461, 30008, 30023, 29999, 30017, 29994, 30011, 1463, 30019, 30021, 29991, 30017, 30021, 30005, 30007, 29999, 30006, 30021, 29999, 30019, 1465, 30008, 30025, 30000, 30008, 30025, 30005, 30007, 29999, 30010, 30020, 29999, 30017, 29994, 30011, 100, 1455, 30019, 30023, 100, 1469, 30007, 29997, 30008, 100, 1461, 30011, 29993, 30017, 30020, 29999, 30017, 30022, 1463, 30011, 30022, 29994, 30019, 29991, 30011, 100, 100, 1463, 30016, 29995, 30010, 30025, 30005, 30006, 29993, 30006, 1012, 1469, 30006, 30025, 1464, 30014, 29996, 30010, 30021, 29999, 30009, 100, 100, 1461, 30008, 30023, 29997, 30006, 29994, 30006, 30023, 29993, 30017, 30022, 29999, 30018, 1460, 30014, 30000, 30019, 29994, 30008, 30021, 30005, 30006, 30021, 1461, 30007, 30025, 30005, 30012, 30022, 29997, 30006, 30025, 29999, 30017, 30022, 1455, 30011, 29997, 30017, 1458, 30006, 30021, 30005, 30019, 1457, 30006, 30023, 29991, 30011, 100, 1463, 30019, 30022, 29995, 30011, 30022, 100, 1461, 30008, 30023, 29999, 30018, 1464, 30008, 30025, 29991, 30010, 30025, 29999, 30017, 30021, 1460, 30006, 29993, 30006, 29999, 30018, 1463, 30006, 29994, 30017, 30023, 29993, 30006, 29999, 30014, 30023, 29999, 30017, 30022, 1455, 30017, 29993, 30007, 29994, 30011, 1468, 30014, 30023, 29991, 30011, 100, 100, 1012, 100, 1010, 1461, 30008, 30023, 29999, 30009, 29992, 30017, 30021, 1455, 30006, 30020, 29997, 30019, 29999, 30010, 1464, 30008, 30021, 29997, 30008, 30022, 1010, 100, 1457, 30017, 30025, 29999, 30018, 1461, 30008, 30022, 30005, 30012, 29991, 30006, 1464, 30008, 30021, 30005, 30007, 1456, 30007, 29994, 30010, 29999, 30011, 29991, 30011, 100, 1010, 1459, 30007, 29992, 30010, 30021, 1464, 30008, 30025, 29999, 30015, 30022, 1468, 30014, 30025, 29999, 30008, 30000, 30009, 1468, 30014, 30025, 29997, 30011, 30020, 29999, 30019, 1463, 30019, 29999, 30008, 30000, 30019, 29991, 30011, 100, 1012, 1026, 7987, 1028]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : 소안항은 조용한 섬으로 인근해안이 청정해역으로 일찍이 김 양식을 해서 높은 소득을 올리고 있으며 바다낚시터로도 유명하다. 항 주변에 설치된 양식장들은 섬사람들의 부지런한 생활상을 고스 란히 담고 있으며 일몰 때 섬의 정경은 바다의 아름다움을 그대로 품고 있는 듯하다. 또한, 섬에는 각시여 전설, 도둑바위 등의 설화가 전해 내려오고 있으며, 매년 정월 풍어제 풍속이 이어지고 있다.<br>\n",
      "Token IDs:  [101, 1461, 30011, 29999, 30006, 30021, 30005, 30006, 30025, 29999, 30017, 30021, 1464, 30011, 29999, 30013, 30025, 30005, 30006, 30021, 1461, 30008, 30023, 29999, 30017, 29994, 30011, 1463, 30019, 30021, 29991, 30017, 30021, 30005, 30007, 29999, 30006, 30021, 29999, 30019, 1465, 30008, 30025, 30000, 30008, 30025, 30005, 30007, 29999, 30010, 30020, 29999, 30017, 29994, 30011, 100, 1455, 30019, 30023, 100, 1469, 30007, 29997, 30008, 100, 1461, 30011, 29993, 30017, 30020, 29999, 30017, 30022, 1463, 30011, 30022, 29994, 30019, 29991, 30011, 100, 100, 1463, 30016, 29995, 30010, 30025, 30005, 30006, 29993, 30006, 1012, 1469, 30006, 30025, 1464, 30014, 29996, 30010, 30021, 29999, 30009, 100, 100, 1461, 30008, 30023, 29997, 30006, 29994, 30006, 30023, 29993, 30017, 30022, 29999, 30018, 1460, 30014, 30000, 30019, 29994, 30008, 30021, 30005, 30006, 30021, 1461, 30007, 30025, 30005, 30012, 30022, 29997, 30006, 30025, 29999, 30017, 30022, 1455, 30011, 29997, 30017, 1458, 30006, 30021, 30005, 30019, 1457, 30006, 30023, 29991, 30011, 100, 1463, 30019, 30022, 29995, 30011, 30022, 100, 1461, 30008, 30023, 29999, 30018, 1464, 30008, 30025, 29991, 30010, 30025, 29999, 30017, 30021, 1460, 30006, 29993, 30006, 29999, 30018, 1463, 30006, 29994, 30017, 30023, 29993, 30006, 29999, 30014, 30023, 29999, 30017, 30022, 1455, 30017, 29993, 30007, 29994, 30011, 1468, 30014, 30023, 29991, 30011, 100, 100, 1012, 100, 1010, 1461, 30008, 30023, 29999, 30009, 29992, 30017, 30021, 1455, 30006, 30020, 29997, 30019, 29999, 30010, 1464, 30008, 30021, 29997, 30008, 30022, 1010, 100, 1457, 30017, 30025, 29999, 30018, 1461, 30008, 30022, 30005, 30012, 29991, 30006, 1464, 30008, 30021, 30005, 30007, 1456, 30007, 29994, 30010, 29999, 30011, 29991, 30011, 100, 1010, 1459, 30007, 29992, 30010, 30021, 1464, 30008, 30025, 29999, 30015, 30022, 1468, 30014, 30025, 29999, 30008, 30000, 30009, 1468, 30014, 30025, 29997, 30011, 30020, 29999, 30019, 1463, 30019, 29999, 30008, 30000, 30019, 29991, 30011, 100, 1012, 1026, 7987, 1028, 102]\n",
      "Max sentence lenght: 10797\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs\n",
    "input_ids =[]\n",
    "\n",
    "# For every sentence..\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "\n",
    "        sent\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0 , now as a list of IDs\n",
    "\n",
    "print('Original :' , sentences[0])\n",
    "print('Token IDs: ', input_ids[0])\n",
    "\n",
    "print('Max sentence lenght:' , max([len(sen) for sen in input_ids]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# We will use some utility function from tensorflow (Tensorflow was my first crush)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 64\n",
    "# Padding the input to the max length that is 64\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0 , truncating='post', padding='post')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Creating the attention masks\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "# For each sentences..\n",
    "\n",
    "for sent in input_ids:\n",
    "    # Create the attention mask\n",
    "    # - If a  token ID is 0 , then it's padding , set the mask to 0.\n",
    "    # - If a  token ID is > 0 , then it's a real token, set the mask to 1.\n",
    "    att_mask = [ int(token_id > 0) for token_id in sent ]\n",
    "\n",
    "    # Store the attention_mask for this sentence\n",
    "    attention_masks.append(att_mask)\n",
    "\n",
    "# We will call the train_test_split() function from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels,  validation_labels = train_test_split(input_ids, labels, random_state=2018 ,test_size=0.1)\n",
    "\n",
    "# Performing same steps on the attention masks\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels , random_state=2018 ,test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  101,  1461, 30011, ..., 30007, 29997, 30008],\n       [  101,  1455, 30010, ..., 30022,   100,  2355],\n       [  101,   100,  1469, ..., 29999, 30017, 29994],\n       ...,\n       [  101,  1465, 30014, ..., 30006, 30021, 29999],\n       [  101,   100,  1455, ..., 29999, 30006, 30020],\n       [  101,  1468, 30011, ..., 30017, 30022,  1455]])"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  101,  1455, 30006, ..., 30014, 30022,   100],\n       [  101,  2871, 29992, ..., 29999, 30014, 30022],\n       [  101,  1457, 30007, ..., 29994, 30011,  1455],\n       ...,\n       [  101,   100,  1463, ...,   100,   100,  1461],\n       [  101,  1459, 30011, ..., 30000, 30008, 30021],\n       [  101,  1463, 30019, ..., 29993, 30009,  1465]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#Converting the input dat ato the tensor , which can be feeded to the model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "train_labels = train_labels.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "validation_labels = validation_labels.values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DATA\\AppData\\Local\\Temp\\ipykernel_9352\\3654936025.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n"
     ]
    }
   ],
   "source": [
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader , RandomSampler , SequentialSampler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating the DataLoader which will help us to load data into the GPU/CPU\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs , train_masks , train_labels)\n",
    "train_sampler = RandomSampler(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader =  DataLoader(train_data , sampler=train_sampler , batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_inputs , validation_masks , validation_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "validation_sampler = SequentialSampler(validation_data)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='monologg/kobert', vocab_size=8002, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model =  BertForSequenceClassification.from_pretrained(\"monologg/kobert\",\n",
    "                                                        num_labels = 128,\n",
    "                                                       output_attentions=False,\n",
    "                                                       output_hidden_states  = False,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    " from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca3a40cb616d4a9b8dceca6b753a1621"
      },
      "application/json": {
       "n": 0,
       "total": 368792146,
       "elapsed": 0.011968135833740234,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model =  BertForSequenceClassification.from_pretrained(\"monologg/kobert\",\n",
    "                                                        num_labels = 128,\n",
    "                                                       output_attentions=False,\n",
    "                                                       output_hidden_states  = False,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5 , eps=1e-8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# 0에서 옵티마이저에 설정된 초기 lr까지 선형적으로 증가하는 워밍업 기간 후에 옵티마이저에 설정된 초기 lr에서 0으로 선형적으로 감소하는 학습률로 스케줄을 생성합니다.\n",
    "# warmup steps까지는 초기 lr로 워밍업하다가 그 이후 부터 감소하는듯\n",
    "# warmup 단계까지는 설정한 learning_rate 까지 num_warmuip_steps 에 비례하여 선형적 으로 상승하는 학습률을 만든다 그 후 감쇠\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= 5 , num_training_steps=total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    }
   ],
   "source": [
    "scheduler.get_lr()\n",
    "scheduler.step()\n",
    "val = scheduler.get_lr()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "2e-05"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "\n",
    "    :param elapsed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Round to the nearest second\n",
    "    elasped_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elasped_rounded))\n",
    "\n",
    "# Let's start the training process\n",
    "import random\n",
    "\n",
    "# This training code is based on the 'run_clue.py' script here\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "\n",
    "loss_values = []\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.998952879581152e-05]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "[2e-05]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.9937238493723853e-05]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.9926778242677826e-05]"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "scheduler\n",
    "import numpy as np\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a stirng hh:mm: ss\n",
    "\n",
    "    :param elapsed:\n",
    "    :return:\n",
    "\n",
    "    \"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format  as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# embedding vocab_size 가 8002 인데 넘어가면 out of index인듯 하다\n",
    "#(word_embeddings): Embedding(8002, 768, padding_idx=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  101,  1463, 30013,  ..., 29999, 30018,  1463],\n        [  101,  3429, 29992,  ..., 30000, 30008, 30020],\n        [  101,  1464, 30008,  ..., 30006, 30021,  1457],\n        ...,\n        [  101,  1465, 30008,  ..., 30011,  1463, 30016],\n        [  101,  1460, 30019,  ...,  1461, 30006, 30025],\n        [  101,  1464, 30008,  ...,  1455, 30007, 29999]], dtype=torch.int32)"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [153]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1522\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1514\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;124;03mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\u001B[39;00m\n\u001B[0;32m   1518\u001B[0m \u001B[38;5;124;03m    If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1519\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1520\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1522\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1532\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1534\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   1536\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:984\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    977\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    982\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 984\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    986\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    988\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    989\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    990\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    991\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m    992\u001B[0m     embedding_output,\n\u001B[0;32m    993\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1001\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1002\u001B[0m )\n\u001B[0;32m   1003\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:215\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    212\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 215\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m token_type_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_type_embeddings(token_type_ids)\n\u001B[0;32m    218\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:2044\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2038\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2039\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2040\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2041\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2042\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2043\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2044\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "outputs = model(data[0],\n",
    "                token_type_ids=None,\n",
    "                attention_mask=data[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  101,  1463, 30013, 30025, 29999, 30019, 30021, 29997, 30019,  1461,\n        30014, 30000, 30019, 29991, 30014,  1457, 30011, 30025, 30001, 30008,\n        30021, 29993, 30011, 30025, 29999, 30009,   100,  1469, 30006, 30021,\n        29999, 30011, 30020,  1455, 30008, 30021, 29995, 30014, 30022, 29999,\n        30018,  1469, 30006, 30021, 29997, 30019, 30020, 29993, 30006, 30025,\n        29999, 30019, 29993, 30006,  1012,  1469, 30006, 30021, 29999, 30011,\n        30020, 29999, 30018,  1463], dtype=torch.int32)"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== EPOCH 1 / 4 ==========\n",
      "Training.....\n",
      "\n",
      " Average traingin loss : 0.00\n",
      " Training epoch took : 0:00:00\n",
      " \n",
      " Running Validation\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [132]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     89\u001B[0m b_input_ids , b_input_masks, b_labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 94\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mb_input_masks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m logits \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    101\u001B[0m logits \u001B[38;5;241m=\u001B[39m logits\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1522\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1514\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;124;03mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\u001B[39;00m\n\u001B[0;32m   1518\u001B[0m \u001B[38;5;124;03m    If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1519\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1520\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1522\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1532\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1534\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   1536\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:984\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    977\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    982\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 984\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    986\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    988\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    989\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    990\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    991\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m    992\u001B[0m     embedding_output,\n\u001B[0;32m    993\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1001\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1002\u001B[0m )\n\u001B[0;32m   1003\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:215\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    212\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 215\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m token_type_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_type_embeddings(token_type_ids)\n\u001B[0;32m    218\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\data\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:2044\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2038\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2039\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2040\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2041\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2042\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2043\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2044\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    #=======================\n",
    "    #   TRAINING\n",
    "    #=======================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=========== EPOCH {:} / {:} ==========\".format(epoch_i+1,epochs))\n",
    "    print(\"Training.....\")\n",
    "\n",
    "\n",
    "    # Measure how long the traingin epochs takes\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step% 40 == 0 and not step ==0:\n",
    "            # Calculate elapsed time in minutes\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress\n",
    "            print(' Batch {:>5,} of {:>5,}. Elapsed : {:} .'.format(step , len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0]\n",
    "            b_input_masks = batch[1]\n",
    "            b_labels = batch[2]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids ,\n",
    "                            token_type_ids=None,\n",
    "                            attention_masks=b_input_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss =  total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\" Average traingin loss : {0:.2f}\".format(avg_train_loss))\n",
    "        print(\" Training epoch took : {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "\n",
    "        print(\" \")\n",
    "        print( \" Running Validation\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        eval_loss , eval_accuracy = 0, 0\n",
    "        nb_eval_steps , nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t for t in batch)\n",
    "\n",
    "            b_input_ids , b_input_masks, b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "\n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_masks)\n",
    "\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.numpy()\n",
    "\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "            eval_accuracy  += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        #Report the final accuracy for this validation run.\n",
    "\n",
    "        print(\" Accuracy : {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\" Validation took: {:}\".format(format_time(time.time()- t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if 'obert' in pt:\n",
    "#     tokenizer = KoBertTokenizer.from_pretrained(pt,  cache_dir='bert_ckpt', do_lower_case=False)\n",
    "#     print('load kobert')\n",
    "# else:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(args.pt)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}